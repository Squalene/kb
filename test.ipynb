{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from kb.include_all import ModelArchiveFromParams\n",
    "\n",
    "from kb.knowbert_utils import KnowBertBatchifier\n",
    "from allennlp.common import Params\n",
    "\n",
    "# contains pretrained model, e.g. for Wordnet+Wikipedia\n",
    "WORDNET_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wordnet_model.tar.gz\"\n",
    "WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_model.tar.gz\"\n",
    "WORDNET_WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_wordnet_model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "params = Params({\"archive_file\": WORDNET_ARCHIVE})#Only contains a dictionnary with a single entry: archive_file:http://...\n",
    "model = ModelArchiveFromParams.from_params(params=params) #P\n",
    "batcher = KnowBertBatchifier(WORDNET_ARCHIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'archive_file': 'https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wordnet_model.tar.gz'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and batcher\n",
    "params = Params({\"archive_file\": WORDNET_ARCHIVE})\n",
    "params.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'kb.knowbert.KnowBert'>\n",
      "<class 'allennlp.common.params.Params'>\n",
      "<class 'kb.knowbert_utils.KnowBertBatchifier'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(params))\n",
    "print(type(batcher))\n",
    "\n",
    "# config = Params.from_file\n",
    "\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_model_type': 'bert-base-uncased',\n",
       " 'do_lower_case': True,\n",
       " 'entity_candidate_generators': {'wordnet': {'entity_file': 'https://allennlp.s3-us-west-2.amazonaws.com/knowbert/wordnet/entities.jsonl',\n",
       "   'type': 'wordnet_mention_generator'}},\n",
       " 'entity_indexers': {'wordnet': {'namespace': 'entity',\n",
       "   'tokenizer': {'type': 'word', 'word_splitter': {'type': 'just_spaces'}},\n",
       "   'type': 'characters_tokenizer'}},\n",
       " 'type': 'bert_tokenizer_and_candidate_generator'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batcher_config = Params.from_file('../knowbert_wordnet_model/config.json')\n",
    "batcher_config['dataset_reader'].as_dict()\n",
    "\n",
    "# candidate_generator_params = _find_key(\n",
    "#             batcher_config['dataset_reader'].as_dict(), 'tokenizer_and_candidate_generator'\n",
    "#         )\n",
    "\n",
    "candidate_generator_params = batcher_config['dataset_reader']['dataset_readers']['language_modeling']['base_reader']['tokenizer_and_candidate_generator'].as_dict()\n",
    "\n",
    "candidate_generator_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Sequence, Union\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from allennlp.data.fields import Field, TextField, ListField, SpanField, ArrayField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data import Token\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer, BasicTokenizer\n",
    "\n",
    "from kb.dict_field import DictField\n",
    "from allennlp.common.registrable import Registrable\n",
    "\n",
    "from kb.common import MentionGenerator\n",
    "\n",
    "start_token = \"[CLS]\"\n",
    "sep_token = \"[SEP]\"\n",
    "\n",
    "def get_empty_candidates():\n",
    "    \"\"\"\n",
    "    The mention generators always return at least one candidate, but signal\n",
    "    it with this special candidate\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"candidate_spans\": [[-1, -1]],\n",
    "        \"candidate_entities\": [[\"@@PADDING@@\"]],\n",
    "        \"candidate_entity_priors\": [[1.0]]\n",
    "    }\n",
    "\n",
    "def truncate_sequence_pair(word_piece_tokens_a, word_piece_tokens_b, max_word_piece_sequence_length):\n",
    "    length_a = sum([len(x) for x in word_piece_tokens_a])\n",
    "    length_b = sum([len(x) for x in word_piece_tokens_b])\n",
    "    while max_word_piece_sequence_length < length_a + length_b:\n",
    "        if length_a < length_b:\n",
    "            discarded = word_piece_tokens_b.pop()\n",
    "            length_b -= len(discarded)\n",
    "        else:\n",
    "            discarded = word_piece_tokens_a.pop()\n",
    "            length_a -= len(discarded)\n",
    "\n",
    "\n",
    "class TokenizerAndCandidateGenerator(Registrable):\n",
    "    pass\n",
    "\n",
    "@TokenizerAndCandidateGenerator.register(\"bert_tokenizer_and_candidate_generator\")\n",
    "class BertTokenizerAndCandidateGenerator(Registrable):\n",
    "    def __init__(self,\n",
    "                 entity_candidate_generators: Dict[str, MentionGenerator],\n",
    "                 entity_indexers: Dict[str, TokenIndexer],\n",
    "                 bert_model_type: str,\n",
    "                 do_lower_case: bool,\n",
    "                 whitespace_tokenize: bool = True,\n",
    "                 max_word_piece_sequence_length: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Note: the fields need to be used with a pre-generated allennlp vocabulary\n",
    "        that contains the entity id namespaces and the bert name space.\n",
    "        entity_indexers = {'wordnet': indexer for wordnet entities,\n",
    "                          'wiki': indexer for wiki entities}\n",
    "        \"\"\"\n",
    "        # load BertTokenizer from huggingface\n",
    "        self.candidate_generators = entity_candidate_generators\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            bert_model_type, do_lower_case=do_lower_case\n",
    "        )\n",
    "        self.bert_word_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "        # Target length should include start and end token\n",
    "        self.max_word_piece_sequence_length = max_word_piece_sequence_length\n",
    "\n",
    "        self._entity_indexers = entity_indexers\n",
    "        # for bert, we'll give an empty token indexer with empty name space\n",
    "        # and do the indexing directly with the bert vocab to bypass\n",
    "        # indexing in the indexer\n",
    "        self._bert_single_id_indexer = {'tokens': SingleIdTokenIndexer('__bert__')}\n",
    "        self.do_lowercase = do_lower_case\n",
    "        self.whitespace_tokenize = whitespace_tokenize\n",
    "        self.dtype = np.float32\n",
    "\n",
    "    def _word_to_word_pieces(self, word):\n",
    "        if self.do_lowercase and word not in self.bert_tokenizer.basic_tokenizer.never_split:\n",
    "            word = word.lower()\n",
    "        return self.bert_tokenizer.wordpiece_tokenizer.tokenize(word)\n",
    "\n",
    "    def tokenize_and_generate_candidates(self, text_a: str, text_b: str = None):\n",
    "        \"\"\"\n",
    "        # run BertTokenizer.basic_tokenizer.tokenize on sentence1 and sentence2 to word tokenization\n",
    "        # generate candidate mentions for each of the generators and for each of sentence1 and 2 from word tokenized text\n",
    "        # run BertTokenizer.wordpiece_tokenizer on sentence1 and sentence2\n",
    "        # truncate length, add [CLS] and [SEP] to word pieces\n",
    "        # compute token offsets\n",
    "        # combine candidate mention spans from sentence1 and sentence2 and remap to word piece indices\n",
    "\n",
    "        returns:\n",
    "\n",
    "        {'tokens': List[str], the word piece strings with [CLS] [SEP]\n",
    "         'segment_ids': List[int] the same length as 'tokens' with 0/1 for sentence1 vs 2\n",
    "         'candidates': Dict[str, Dict[str, Any]],\n",
    "            {'wordnet': {'candidate_spans': List[List[int]],\n",
    "                         'candidate_entities': List[List[str]],\n",
    "                         'candidate_entity_prior': List[List[float]],\n",
    "                         'segment_ids': List[int]},\n",
    "             'wiki': ...}\n",
    "        }\n",
    "        \"\"\"\n",
    "        offsets_a, grouped_wp_a, tokens_a = self._tokenize_text(text_a)\n",
    "\n",
    "        if text_b is not None:\n",
    "            offsets_b, grouped_wp_b, tokens_b = self._tokenize_text(text_b)\n",
    "            truncate_sequence_pair(grouped_wp_a, grouped_wp_b, self.max_word_piece_sequence_length - 3)\n",
    "            offsets_b = offsets_b[:len(grouped_wp_b)]\n",
    "            tokens_b = tokens_b[:len(grouped_wp_b)]\n",
    "            instance_b = self._generate_sentence_entity_candidates(tokens_b, offsets_b)\n",
    "            word_piece_tokens_b = [word_piece for word in grouped_wp_b for word_piece in word]\n",
    "        else:\n",
    "            length_a = sum([len(x) for x in grouped_wp_a])\n",
    "            while self.max_word_piece_sequence_length - 2 < length_a:\n",
    "                discarded = grouped_wp_a.pop()\n",
    "                length_a -= len(discarded)\n",
    "\n",
    "        word_piece_tokens_a = [word_piece for word in grouped_wp_a for word_piece in word]\n",
    "        offsets_a = offsets_a[:len(grouped_wp_a)]\n",
    "        tokens_a = tokens_a[:len(grouped_wp_a)]\n",
    "        instance_a = self._generate_sentence_entity_candidates(tokens_a, offsets_a)\n",
    "\n",
    "        # If we got 2 sentences.\n",
    "        if text_b is not None:\n",
    "            # Target length should include start and two end tokens, and then be divided equally between both sentences\n",
    "            # Note that this will result in potentially shorter documents than original target length,\n",
    "            # if one (or both) of the sentences are shorter than half the target length.\n",
    "            tokens = [start_token] + word_piece_tokens_a + [sep_token] + word_piece_tokens_b + [sep_token]\n",
    "            segment_ids = (len(word_piece_tokens_a) + 2) * [0] + (len(word_piece_tokens_b) + 1) * [1]\n",
    "            offsets_a = [x + 1 for x in offsets_a]\n",
    "            offsets_b = [x + 2 + len(word_piece_tokens_a) for x in offsets_b]\n",
    "        # Single sentence\n",
    "        else:\n",
    "            tokens = [start_token] + word_piece_tokens_a + [sep_token]\n",
    "            segment_ids = len(tokens) * [0]\n",
    "            offsets_a = [x + 1 for x in offsets_a]\n",
    "            offsets_b = None\n",
    "\n",
    "        for name in instance_a.keys():\n",
    "            for span in instance_a[name]['candidate_spans']:\n",
    "                span[0] += 1\n",
    "                span[1] += 1\n",
    "\n",
    "        fields: Dict[str, Sequence] = {}\n",
    "\n",
    "        # concatanating both sentences (for both tokens and ids)\n",
    "        if text_b is None:\n",
    "            candidates = instance_a\n",
    "        else:\n",
    "            candidates: Dict[str, Field] = {}\n",
    "\n",
    "            # Merging candidate lists for both sentences.\n",
    "            for entity_type in instance_b:\n",
    "                candidate_instance_a = instance_a[entity_type]\n",
    "                candidate_instance_b = instance_b[entity_type]\n",
    "\n",
    "                candidates[entity_type] = {}\n",
    "\n",
    "                for span in candidate_instance_b['candidate_spans']:\n",
    "                    span[0] += len(word_piece_tokens_a) + 2\n",
    "                    span[1] += len(word_piece_tokens_a) + 2\n",
    "\n",
    "                # Merging each of the fields.\n",
    "                for key in ['candidate_entities', 'candidate_spans', 'candidate_entity_priors']:\n",
    "                    candidates[entity_type][key] = candidate_instance_a[key] + candidate_instance_b[key]\n",
    "\n",
    "\n",
    "        for entity_type in candidates.keys():\n",
    "            # deal with @@PADDING@@\n",
    "            if len(candidates[entity_type]['candidate_entities']) == 0:\n",
    "                candidates[entity_type] = get_empty_candidates()\n",
    "            else:\n",
    "                padding_indices = []\n",
    "                has_entity = False\n",
    "                for cand_i, candidate_list in enumerate(candidates[entity_type]['candidate_entities']):\n",
    "                    if candidate_list == [\"@@PADDING@@\"]:\n",
    "                        padding_indices.append(cand_i)\n",
    "                        candidates[entity_type][\"candidate_spans\"][cand_i] = [-1, -1]\n",
    "                    else:\n",
    "                        has_entity = True\n",
    "                indices_to_remove = []\n",
    "                if has_entity and len(padding_indices) > 0:\n",
    "                    # remove all the padding entities since have some valid\n",
    "                    indices_to_remove = padding_indices\n",
    "                elif len(padding_indices) > 0:\n",
    "                    assert len(padding_indices) == len(candidates[entity_type]['candidate_entities'])\n",
    "                    indices_to_remove = padding_indices[1:]\n",
    "                for ind in reversed(indices_to_remove):\n",
    "                    del candidates[entity_type][\"candidate_spans\"][ind]\n",
    "                    del candidates[entity_type][\"candidate_entities\"][ind]\n",
    "                    del candidates[entity_type][\"candidate_entity_priors\"][ind]\n",
    "\n",
    "        # get the segment ids for the spans\n",
    "        for key, cands in candidates.items():\n",
    "            span_segment_ids = []\n",
    "            for candidate_span in cands['candidate_spans']:\n",
    "                span_segment_ids.append(segment_ids[candidate_span[0]])\n",
    "            candidates[key]['candidate_segment_ids'] = span_segment_ids\n",
    "\n",
    "        fields['tokens'] = tokens\n",
    "        fields['segment_ids'] = segment_ids\n",
    "        fields['candidates'] = candidates\n",
    "        fields['offsets_a'] = offsets_a\n",
    "        fields['offsets_b'] = offsets_b\n",
    "        return fields\n",
    "\n",
    "    def _tokenize_text(self, text):\n",
    "        if self.whitespace_tokenize:\n",
    "            tokens = text.split()\n",
    "        else:\n",
    "            tokens = self.bert_word_tokenizer.tokenize(text)\n",
    "\n",
    "        word_piece_tokens = []\n",
    "        offsets = [0]\n",
    "        for token in tokens:\n",
    "            #NOTE: lowercase if necessary and tokenize\n",
    "            word_pieces = self._word_to_word_pieces(token)\n",
    "            offsets.append(offsets[-1] + len(word_pieces))\n",
    "            word_piece_tokens.append(word_pieces)\n",
    "        del offsets[0]\n",
    "        return offsets, word_piece_tokens, tokens\n",
    "\n",
    "    def _generate_sentence_entity_candidates(self, tokens, offsets):\n",
    "        \"\"\"\n",
    "        Tokenize sentence, trim it to the target length, and generate entity candidates.\n",
    "        :param sentence\n",
    "        :param target_length: The length of the output sentence in terms of word pieces.\n",
    "        :return: Dict[str, Dict[str, Any]],\n",
    "            {'wordnet': {'candidate_spans': List[List[int]],\n",
    "                         'candidate_entities': List[List[str]],\n",
    "                         'candidate_entity_priors': List[List[float]]},\n",
    "             'wiki': ...}\n",
    "\n",
    "        \"\"\"\n",
    "        assert len(tokens) == len(offsets), f'Length of tokens {len(tokens)} must equal that of offsets {len(offsets)}.'\n",
    "        entity_instances = {}\n",
    "        for name, mention_generator in self.candidate_generators.items():\n",
    "            entity_instances[name] = mention_generator.get_mentions_raw_text(' '.join(tokens), whitespace_tokenize=True)\n",
    "\n",
    "        for name, entities in entity_instances.items():\n",
    "            candidate_spans = entities[\"candidate_spans\"]\n",
    "            adjusted_spans = []\n",
    "            for start, end in candidate_spans:\n",
    "                if 0 < start:\n",
    "                    adjusted_span = [offsets[start - 1], offsets[end] - 1]\n",
    "                else:\n",
    "                    adjusted_span = [0, offsets[end] - 1]\n",
    "                adjusted_spans.append(adjusted_span)\n",
    "            entities['candidate_spans'] = adjusted_spans\n",
    "            entity_instances[name] = entities\n",
    "        return entity_instances\n",
    "\n",
    "    def convert_tokens_candidates_to_fields(self, tokens_and_candidates):\n",
    "        \"\"\"\n",
    "        tokens_and_candidates is the return from a previous call to\n",
    "        generate_sentence_entity_candidates.  Converts the dict to\n",
    "        a dict of fields usable with allennlp.\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "\n",
    "        fields['tokens'] = TextField(\n",
    "                [Token(t, text_id=self.bert_tokenizer.vocab[t])\n",
    "                    for t in tokens_and_candidates['tokens']],\n",
    "                token_indexers=self._bert_single_id_indexer\n",
    "        )\n",
    "\n",
    "        fields['segment_ids'] = ArrayField(\n",
    "            np.array(tokens_and_candidates['segment_ids']), dtype=np.int\n",
    "        )\n",
    "\n",
    "        all_candidates = {}\n",
    "        for key, entity_candidates in tokens_and_candidates['candidates'].items():\n",
    "            # pad the prior to create the array field\n",
    "            # make a copy to avoid modifying the input\n",
    "            candidate_entity_prior = copy.deepcopy(\n",
    "                    entity_candidates['candidate_entity_priors']\n",
    "            )\n",
    "            max_cands = max(len(p) for p in candidate_entity_prior)\n",
    "            for p in candidate_entity_prior:\n",
    "                if len(p) < max_cands:\n",
    "                    p.extend([0.0] * (max_cands - len(p)))\n",
    "            np_prior = np.array(candidate_entity_prior)\n",
    "\n",
    "            candidate_fields = {\n",
    "                \"candidate_entity_priors\": ArrayField(np_prior, dtype=self.dtype),\n",
    "                \"candidate_entities\": TextField(\n",
    "                    [Token(\" \".join(candidate_list)) for candidate_list in entity_candidates[\"candidate_entities\"]],\n",
    "                    token_indexers={'ids': self._entity_indexers[key]}),\n",
    "                \"candidate_spans\": ListField(\n",
    "                    [SpanField(span[0], span[1], fields['tokens']) for span in\n",
    "                    entity_candidates['candidate_spans']]\n",
    "                ),\n",
    "                \"candidate_segment_ids\": ArrayField(\n",
    "                    np.array(entity_candidates['candidate_segment_ids']), dtype=np.int\n",
    "        )\n",
    "            }\n",
    "            all_candidates[key] = DictField(candidate_fields)\n",
    "\n",
    "        fields[\"candidates\"] = DictField(all_candidates)\n",
    "\n",
    "        return fields\n",
    "\n",
    "\n",
    "@TokenizerAndCandidateGenerator.register(\"pretokenized\")\n",
    "class PretokenizedTokenizerAndCandidateGenerator(BertTokenizerAndCandidateGenerator):\n",
    "    \"\"\"\n",
    "    Simple modification to the ``BertTokenizerAndCandidateGenerator``. We assume data comes\n",
    "    pre-tokenized, so only wordpiece splitting is performed.\n",
    "\n",
    "    # TODO: mypy is not going to like us calling ``tokenize_and_generate_candidates()`` on lists\n",
    "    # instead of strings. Maybe update type annotations in ``BertTokenizerAndCandidateGenerator``?\n",
    "    \"\"\"\n",
    "    def _tokenize_text(self, tokens: List[str]):\n",
    "        word_piece_tokens = []\n",
    "        offsets = [0]\n",
    "        for token in tokens:\n",
    "            # Stupid hack\n",
    "            if token in ['[SEP]', '[MASK]']:\n",
    "                word_pieces = [token]\n",
    "            else:\n",
    "                word_pieces = self._word_to_word_pieces(token)\n",
    "            offsets.append(offsets[-1] + len(word_pieces))\n",
    "            word_piece_tokens.append(word_pieces)\n",
    "        del offsets[0]\n",
    "        return offsets, word_piece_tokens, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union, List\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.data import Instance, DataIterator, Vocabulary\n",
    "from allennlp.common.file_utils import cached_path\n",
    "\n",
    "\n",
    "from kb.include_all import TokenizerAndCandidateGenerator\n",
    "from kb.bert_pretraining_reader import replace_candidates_with_mask_entity\n",
    "\n",
    "\n",
    "\n",
    "def _extract_config_from_archive(model_archive):\n",
    "    import tarfile\n",
    "    import tempfile\n",
    "    import os\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        with tarfile.open(model_archive, 'r:gz') as archive:\n",
    "            archive.extract('config.json', path=tmp)\n",
    "            config = Params.from_file(os.path.join(tmp, 'config.json'))\n",
    "    return config\n",
    "\n",
    "#NOTE: Recursively traverse dictionary to find the value corresponding to a key\n",
    "def _find_key(d, key):\n",
    "    val = None\n",
    "    stack = [d.items()]\n",
    "    while len(stack) > 0 and val is None:\n",
    "        s = stack.pop()\n",
    "        for k, v in s:\n",
    "            if k == key:\n",
    "                val = v\n",
    "                break\n",
    "            elif isinstance(v, dict):\n",
    "                stack.append(v.items())\n",
    "    return val\n",
    "\n",
    "class KnowBertBatchifierCustom:\n",
    "    \"\"\"\n",
    "    Takes a list of sentence strings and returns a tensor dict usable with\n",
    "    a KnowBert model\n",
    "    \"\"\"\n",
    "    def __init__(self, candidate_generator_params,vocab_params, batch_size=32,\n",
    "                       masking_strategy=None,\n",
    "                       wordnet_entity_file=None):\n",
    "\n",
    "        # get bert_tokenizer_and_candidate_generator\n",
    "        #config = _extract_config_from_archive(cached_path(model_archive))\n",
    "\n",
    "        # look for the bert_tokenizers and candidate_generator\n",
    "        #NOTE: Contains config info for tokenizer and candidate_generator\n",
    "        # candidate_generator_params = _find_key(\n",
    "        #     config['dataset_reader'].as_dict(), 'tokenizer_and_candidate_generator'\n",
    "        # )\n",
    "\n",
    "        if wordnet_entity_file is not None:\n",
    "            candidate_generator_params['entity_candidate_generators']['wordnet']['entity_file'] = wordnet_entity_file\n",
    "\n",
    "        self.tokenizer_and_candidate_generator = TokenizerAndCandidateGenerator.\\\n",
    "                from_params(Params(candidate_generator_params))\n",
    "                \n",
    "        self.tokenizer_and_candidate_generator.whitespace_tokenize = False\n",
    "\n",
    "        assert masking_strategy is None or masking_strategy == 'full_mask'\n",
    "\n",
    "        self.masking_strategy = masking_strategy\n",
    "\n",
    "        # need bert_tokenizer_and_candidate_generator\n",
    "        # if vocab_dir is not None:\n",
    "        #     vocab_params = Params({\"directory_path\": vocab_dir})\n",
    "        # else:\n",
    "        #     vocab_params = config['vocabulary']\n",
    "\n",
    "        self.vocab = Vocabulary.from_params(vocab_params)\n",
    "\n",
    "        self.iterator = DataIterator.from_params(\n",
    "            Params({\"type\": \"basic\", \"batch_size\": batch_size})\n",
    "        )\n",
    "        self.iterator.index_with(self.vocab)\n",
    "\n",
    "    def _replace_mask(self, s):\n",
    "        return s.replace('[MASK]', ' [MASK] ')\n",
    "\n",
    "    def iter_batches(self, sentences_or_sentence_pairs: Union[List[str], List[List[str]]], verbose=True):\n",
    "        # create instances\n",
    "        instances = []\n",
    "        for sentence_or_sentence_pair in sentences_or_sentence_pairs:\n",
    "            if isinstance(sentence_or_sentence_pair, list):\n",
    "                assert len(sentence_or_sentence_pair) == 2\n",
    "                tokens_candidates = self.tokenizer_and_candidate_generator.\\\n",
    "                        tokenize_and_generate_candidates(\n",
    "                                self._replace_mask(sentence_or_sentence_pair[0]),\n",
    "                                self._replace_mask(sentence_or_sentence_pair[1]))\n",
    "            else:\n",
    "                tokens_candidates = self.tokenizer_and_candidate_generator.\\\n",
    "                        tokenize_and_generate_candidates(self._replace_mask(sentence_or_sentence_pair))\n",
    "\n",
    "            if verbose:\n",
    "                print(self._replace_mask(sentence_or_sentence_pair))\n",
    "                print(tokens_candidates['tokens'])\n",
    "\n",
    "            # now modify the masking if needed\n",
    "            if self.masking_strategy == 'full_mask':\n",
    "                # replace the mask span with a @@mask@@ span\n",
    "                masked_indices = [index for index, token in enumerate(tokens_candidates['tokens'])\n",
    "                      if token == '[MASK]']\n",
    "\n",
    "                spans_to_mask = set([(i, i) for i in masked_indices])\n",
    "                replace_candidates_with_mask_entity(\n",
    "                        tokens_candidates['candidates'], spans_to_mask\n",
    "                )\n",
    "\n",
    "                # now make sure the spans are actually masked\n",
    "                for key in tokens_candidates['candidates'].keys():\n",
    "                    for span_to_mask in spans_to_mask:\n",
    "                        found = False\n",
    "                        for span in tokens_candidates['candidates'][key]['candidate_spans']:\n",
    "                            if tuple(span) == tuple(span_to_mask):\n",
    "                                found = True\n",
    "                        if not found:\n",
    "                            tokens_candidates['candidates'][key]['candidate_spans'].append(list(span_to_mask))\n",
    "                            tokens_candidates['candidates'][key]['candidate_entities'].append(['@@MASK@@'])\n",
    "                            tokens_candidates['candidates'][key]['candidate_entity_priors'].append([1.0])\n",
    "                            tokens_candidates['candidates'][key]['candidate_segment_ids'].append(0)\n",
    "                            # hack, assume only one sentence\n",
    "                            assert not isinstance(sentence_or_sentence_pair, list)\n",
    "\n",
    "\n",
    "            fields = self.tokenizer_and_candidate_generator.\\\n",
    "                convert_tokens_candidates_to_fields(tokens_candidates)\n",
    "\n",
    "            instances.append(Instance(fields))\n",
    "\n",
    "\n",
    "        for batch in self.iterator(instances, num_epochs=1, shuffle=False):\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens are equal: True\n",
      "Segment ids are equal: True\n",
      "Candidate entity priors are equal: True\n",
      "Candidate entity ids are equal: True\n",
      "Candidate spans are equal: True\n",
      "Candidate segment ids are equal: True\n"
     ]
    }
   ],
   "source": [
    "def test_batcher(original_batcher,custom_batcher):\n",
    "    sentences = [\"Paris is located in France.\", \"Michael Jackson is a great music singer\"]\n",
    "    # batcher takes raw untokenized sentences\n",
    "    # and yields batches of tensors needed to run KnowBert\n",
    "    for original_batch,custom_batch in zip(original_batcher.iter_batches(sentences, verbose=False),custom_batcher.iter_batches(sentences, verbose=False)):\n",
    "\n",
    "        print(f\"Tokens are equal: {torch.equal(original_batch['tokens']['tokens'],custom_batch['tokens']['tokens'])}\")\n",
    "        print(f\"Segment ids are equal: {torch.equal(original_batch['segment_ids'],custom_batch['segment_ids'])}\")\n",
    "        original_wordnet_kb = original_batch['candidates']['wordnet']\n",
    "        custom_wordnet_kb = custom_batch['candidates']['wordnet']\n",
    "        print(f\"Candidate entity priors are equal: {torch.equal(original_wordnet_kb['candidate_entity_priors'],custom_wordnet_kb['candidate_entity_priors'])}\")\n",
    "        print(f\"Candidate entity ids are equal: {torch.equal(original_wordnet_kb['candidate_entities']['ids'],custom_wordnet_kb['candidate_entities']['ids'])}\")\n",
    "        print(f\"Candidate spans are equal: {torch.equal(original_wordnet_kb['candidate_spans'],custom_wordnet_kb['candidate_spans'])}\")\n",
    "        print(f\"Candidate segment ids are equal: {torch.equal(original_wordnet_kb['candidate_segment_ids'],custom_wordnet_kb['candidate_segment_ids'])}\")\n",
    "\n",
    "batcher_config = Params.from_file('../knowbert_wordnet_model/config.json')\n",
    "candidate_generator_params = batcher_config['dataset_reader']['dataset_readers']['language_modeling']['base_reader']['tokenizer_and_candidate_generator'].as_dict()\n",
    "vocab_param = batcher_config['vocabulary']\n",
    "custom_batcher = KnowBertBatchifierCustom(candidate_generator_params,vocab_param)\n",
    "test_batcher(batcher,custom_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is located in France.\n",
      "['[CLS]', 'paris', 'is', 'located', 'in', 'france', '.', '[SEP]']\n",
      "Michael Jackson is a great music singer\n",
      "['[CLS]', 'michael', 'jackson', 'is', 'a', 'great', 'music', 'singer', '[SEP]']\n",
      "\n",
      "Input\n",
      "\n",
      "Batch: dict_keys(['tokens', 'segment_ids', 'candidates'])\n",
      "Tokens shape torch.Size([2, 9])\n",
      "Segment ids shape: torch.Size([2, 9])\n",
      "Wordnet kb: dict_keys(['candidate_entity_priors', 'candidate_entities', 'candidate_spans', 'candidate_segment_ids'])\n",
      "Candidate entity_priors shape: torch.Size([2, 8, 14])\n",
      "Candidate entities ids shape: torch.Size([2, 8, 14])\n",
      "Candidate span shape: torch.Size([2, 8, 2])\n",
      "Candidate segments_ids shape: torch.Size([2, 8])\n",
      "\n",
      "Output\n",
      "\n",
      "Model output keys: dict_keys(['wordnet', 'loss', 'pooled_output', 'contextual_embeddings'])\n",
      "Output wordnet keys: dict_keys(['entity_attention_probs', 'linking_scores'])\n",
      "Output wordnet entity_attention_probs shape: torch.Size([2, 4, 9, 8])\n",
      "Output wordnet linking_scores shape: torch.Size([2, 8, 14])\n",
      "Output loss: 0.0\n",
      "Pooled output shape: torch.Size([2, 768])\n",
      "Contextual embeddings: torch.Size([2, 9, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Paris is located in France.\", \"Michael Jackson is a great music singer\"]\n",
    "# batcher takes raw untokenized sentences\n",
    "# and yields batches of tensors needed to run KnowBert\n",
    "for i,batch in enumerate(batcher.iter_batches(sentences, verbose=True)):\n",
    "\n",
    "    print(f\"\\nInput\\n\")\n",
    "    print(f\"Batch: {batch.keys()}\") #Batch contains {tokens,segment_ids,candidates}\n",
    "    #tokens: Tensor of tokens indices (used to idx an embedding) => because a batch contains multiple\n",
    "    #sentences with varying # of tokens, all tokens tensors are padded with zeros \n",
    "    #shape: (batch_size (#sentences), max_seq_len)\n",
    "    #print(batch['tokens'])#dict with only 'tokens'\n",
    "    print(f\"Tokens shape {batch['tokens']['tokens'].shape}\")\n",
    "    #Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "    #shape: (batch_size,max_seq_len)\n",
    "    print(f\"Segment ids shape: {batch['segment_ids'].shape}\")\n",
    "\n",
    "    #Dict with only wordnet\n",
    "    #Candidates: stores for multiple knowledge base, the entities detected using this knowledge base\n",
    "    wordnet_kb = batch['candidates']['wordnet']\n",
    "    print(f\"Wordnet kb: {wordnet_kb.keys()}\")\n",
    "\n",
    "    \n",
    "    #Stores for each detected entities, a list of candidate KB entities that correspond to it\n",
    "    #Priors: correctness probabilities estimated by the entity linker (sum to 1 (or 0 if padding) on axis 2)\n",
    "    #Adds 0 padding to axis 1 when there is less detected entities in the sentence than in the max sentence\n",
    "    #Adds 0 padding to axis 2 when there is less detected KB entities for an entity in the sentence than in the max candidate KB entities entity\n",
    "    #shape:(batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entity_priors shape: {wordnet_kb['candidate_entity_priors'].shape}\")\n",
    "    #Ids of the KB candidate entities + 0 padding on axis 1 or 2 if necessary\n",
    "    #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entities ids shape: {wordnet_kb['candidate_entities']['ids'].shape}\")\n",
    "    #Spans of which sequence of tokens correspond to an entity in the sentence, eg: [1,2] for Michael Jackson (both bounds are included)\n",
    "    \n",
    "    #Padding with [-1,-1] when no more detected entities\n",
    "    #shape: (batch_size, max # detected entities, 2)\n",
    "    print(f\"Candidate span shape: {wordnet_kb['candidate_spans'].shape}\")\n",
    "\n",
    "    #For each sentence entity, indicate to which segment ids it corresponds to\n",
    "    #shape: (batch_size, max # detected entities)\n",
    "    print(f\"Candidate segments_ids shape: {wordnet_kb['candidate_segment_ids'].shape}\")\n",
    "\n",
    "    #model(**batch) <=> model(tokens = batch['tokens'],segment_ids=batch['segment_ids'],candidates=batch['candidates']) \n",
    "    model_output = model(**batch)\n",
    "    \n",
    "    print(f\"\\nOutput\\n\")\n",
    "    print(f\"Model output keys: {model_output.keys()}\")\n",
    "    print(f\"Output wordnet keys: {model_output['wordnet'].keys()}\")\n",
    "    #Span attention layers scores for wordnet KB\n",
    "    #shape: (batch_size,?,max_seq_len,max # detected entities)\n",
    "    print(f\"Output wordnet entity_attention_probs shape: {model_output['wordnet']['entity_attention_probs'].shape}\")\n",
    "    #Entity linker score for each text entity and possible KB entity, -1.0000e+04 padding in case of no score\n",
    "    #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Output wordnet linking_scores shape: {model_output['wordnet']['linking_scores'].shape}\")\n",
    "    \n",
    "    #Scalar indicating loss over this batch (0 if not training?)\n",
    "    print(f\"Output loss: {model_output['loss']}\")\n",
    "\n",
    "    #Final CLS embedding for each sentence of batch\n",
    "    # shape: (batch_size, hidden_size) \n",
    "    print(f\"Pooled output shape: {model_output['pooled_output'].shape}\")\n",
    "\n",
    "    #For each tokens, its final embeddings\n",
    "    #Important!!!, still predicts something for 0 padded tokens => ignore (or 0 padding <=> MASK???)\n",
    "    print(f\"Contextual embeddings: {model_output['contextual_embeddings'].shape}\")\n",
    "\n",
    "    #TODO: see how to add masking => 0 idx tokens embedding?\n",
    "    #TODO: See how to extract from final embeddings the actual predicted tokens\n",
    "    #TODO: copy paste all allennlp dependencies in an allennlp.py file that contains all classes => get rid of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #TODO: see how to add masking => 0 idx tokens embedding?\n",
    "    #TODO: See how to extract from final embeddings the actual predicted tokens\n",
    "    #TODO: copy paste all allennlp dependencies in an allennlp.py file that contains all classes => get rid of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_bert.bert.embeddings.word_embeddings.weight:torch.Size([30522, 768])\n",
      "pretrained_bert.bert.embeddings.position_embeddings.weight:torch.Size([512, 768])\n",
      "pretrained_bert.bert.embeddings.token_type_embeddings.weight:torch.Size([2, 768])\n",
      "pretrained_bert.bert.embeddings.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.embeddings.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.0.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.0.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.1.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.1.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.2.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.2.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.3.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.3.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.4.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.4.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.5.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.5.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.6.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.6.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.7.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.7.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.8.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.8.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.9.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.9.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.10.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.10.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.11.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.11.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.pooler.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.pooler.dense.bias:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.bias:torch.Size([30522])\n",
      "pretrained_bert.cls.predictions.transform.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.cls.predictions.transform.dense.bias:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.transform.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.transform.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.cls.seq_relationship.weight:torch.Size([2, 768])\n",
      "pretrained_bert.cls.seq_relationship.bias:torch.Size([2])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.weight:torch.Size([1, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.bias:torch.Size([1])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.bert_to_kg_projector.weight:torch.Size([200, 768])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.bert_to_kg_projector.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.projected_span_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.projected_span_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.kg_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.kg_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.pos_embeddings.weight:torch.Size([117663, 25])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.entity_embeddings.weight:torch.Size([117663, 2248])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.proj_feed_forward.weight:torch.Size([200, 2273])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.proj_feed_forward.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_1.weight:torch.Size([100, 2])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_1.bias:torch.Size([100])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_2.weight:torch.Size([1, 100])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_2.bias:torch.Size([1])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.query.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.query.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.key.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.key.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.value.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.value.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.dense.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.intermediate.dense.weight:torch.Size([1024, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.intermediate.dense.bias:torch.Size([1024])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.dense.weight:torch.Size([200, 1024])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.weighted_entity_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.weighted_entity_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.query.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.query.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.key.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.key.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.value.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.value.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.dense.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.intermediate.dense.weight:torch.Size([1024, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.intermediate.dense.bias:torch.Size([1024])\n",
      "wordnet_soldered_kg.span_attention_layer.output.dense.weight:torch.Size([200, 1024])\n",
      "wordnet_soldered_kg.span_attention_layer.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.output_layer_norm.weight:torch.Size([768])\n",
      "wordnet_soldered_kg.output_layer_norm.bias:torch.Size([768])\n",
      "wordnet_soldered_kg.kg_to_bert_projection.weight:torch.Size([768, 200])\n",
      "wordnet_soldered_kg.kg_to_bert_projection.bias:torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}:{param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9797c430b9b8ca5d4cad34220fe2c597e42ba7691ef10261f4554305aef3ef0a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('knowbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
