{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTokenizer params None False None None\n",
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object at 0x7f63c1e79be0> None None 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from kb.include_all import ModelArchiveFromParams\n",
    "\n",
    "from kb.knowbert_utils import KnowBertBatchifier\n",
    "from allennlp.common import Params\n",
    "\n",
    "# contains pretrained model, e.g. for Wordnet+Wikipedia\n",
    "WORDNET_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wordnet_model.tar.gz\"\n",
    "WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_model.tar.gz\"\n",
    "WORDNET_WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_wordnet_model.tar.gz\"\n",
    "\n",
    "WORDNET_FOLDER = '../knowbert_wordnet_model/'\n",
    "WORDNET_LINKER_FOLDER = WORDNET_FOLDER + 'entity_linker/'\n",
    "WORDNET_LINKER_EMBEDDING_FILE = WORDNET_LINKER_FOLDER + 'wordnet_synsets_mask_null_vocab_embeddings_tucker_gensen.hdf5'\n",
    "WORDNET_LINKER_ENTITY_FILE = WORDNET_LINKER_FOLDER + 'entities.jsonl'\n",
    "WORDNET_LINKER_VOCAB_FILE = WORDNET_LINKER_FOLDER + 'wordnet_synsets_mask_null_vocab.txt'\n",
    "\n",
    "\n",
    "WORDNET_MODEL_STATE_DICT_FILE = WORDNET_FOLDER+ 'weights.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set(original_model,batcher):\n",
    "\n",
    "    test_sentences = [[\"Paris is located in [MASK].\", \"Michael [MASK] is a great music singer\"],\n",
    "                [\"The Louvre contains the Mona Lisa\", \"The Amazon river is in Brazil\"],\n",
    "                \"Donald Duck is a cartoon character\",\n",
    "                [\"Hayao Miyazaki is the co-founder of Studio Ghibli and a renowned anime film maker\",\n",
    "                \"The Alpine ibex is one of Switzerland's most famous animal along its grazing cows\"]]\n",
    "                \n",
    "    original_model.eval()\n",
    "    test_set = []\n",
    "    for batch in batcher.iter_batches(test_sentences, verbose=False):\n",
    "        test_case = {}\n",
    "        test_case['input']=batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_case['expected']=original_model(**batch)\n",
    "        test_set.append(test_case)\n",
    "\n",
    "    return test_set\n",
    "\n",
    "def model_correct(custom_model,test_set):\n",
    "    custom_model.eval()\n",
    "    for test_case in test_set:\n",
    "        with torch.no_grad():\n",
    "            custom_output = custom_model(**test_case[\"input\"])\n",
    "\n",
    "        expected_output = test_case[\"expected\"]\n",
    "\n",
    "        for key in test_case[\"expected\"].keys():\n",
    "            if(key in ['wiki','wordnet']):\n",
    "                print(f\"{key} entity_attention_probs are equal: {torch.equal(expected_output[key]['entity_attention_probs'],custom_output[key]['entity_attention_probs'])}\")\n",
    "\n",
    "                print(f\"{key} output linking scores are equal: {torch.equal(expected_output[key]['linking_scores'],custom_output[key]['linking_scores'])}\")\n",
    "            else:\n",
    "                if(key=='loss'):\n",
    "                    print(f\"{key} are equal : {expected_output[key]==custom_output[key]}\")\n",
    "                else:\n",
    "                    print(f\"{key} are equal : {torch.equal(expected_output[key],custom_output[key])}\")\n",
    "\n",
    "                    print(f\"{key} are equal : {torch.equal(expected_output[key],custom_output[key])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object at 0x7f63c1e79be0> None None 0\n",
      "BertTokenizerAndCandidateGenerator params\n",
      "{'wordnet': <kb.wordnet.WordNetCandidateMentionGenerator object at 0x7f62e7358b70>}\n",
      "{'wordnet': <allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer object at 0x7f62e7358c50>}\n",
      "bert-base-uncased\n",
      "True\n",
      "True\n",
      "512\n",
      "Vocab: <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "Soldered kg: {'wordnet': SolderedKG(\n",
      "  (entity_linker): EntityLinkingWithCandidateMentions(\n",
      "    (loss): NLLLoss()\n",
      "    (_log_softmax): LogSoftmax()\n",
      "    (disambiguator): EntityDisambiguator(\n",
      "      (span_extractor): SelfAttentiveSpanExtractor(\n",
      "        (_global_attention): TimeDistributed(\n",
      "          (_module): Linear(in_features=200, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (bert_to_kg_projector): Linear(in_features=768, out_features=200, bias=True)\n",
      "      (projected_span_layer_norm): BertLayerNorm()\n",
      "      (kg_layer_norm): BertLayerNorm()\n",
      "      (entity_embeddings): WordNetAllEmbedding(\n",
      "        (pos_embeddings): Embedding(117663, 25)\n",
      "        (entity_embeddings): Embedding(117663, 2248, padding_idx=0)\n",
      "        (proj_feed_forward): Linear(in_features=2273, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dot_attention_with_prior): DotAttentionWithPrior(\n",
      "        (out_layer_1): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (out_layer_2): Linear(in_features=100, out_features=1, bias=True)\n",
      "      )\n",
      "      (span_encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=200, out_features=200, bias=True)\n",
      "                (key): Linear(in_features=200, out_features=200, bias=True)\n",
      "                (value): Linear(in_features=200, out_features=200, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=200, out_features=200, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=200, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=200, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weighted_entity_layer_norm): BertLayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (span_attention_layer): SpanAttentionLayer(\n",
      "    (attention): SpanAttention(\n",
      "      (attention): SpanWordAttention(\n",
      "        (query): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (key): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (value): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=200, out_features=1024, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=200, bias=True)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer_norm): BertLayerNorm()\n",
      "  (kg_to_bert_projection): Linear(in_features=200, out_features=768, bias=True)\n",
      ")}\n",
      "Soldered layers: <allennlp.common.params.Params object at 0x7f63b2c90240>\n",
      "bert model name bert-base-uncased\n",
      "mode None\n",
      "model archive None\n",
      "strict_load_archive True\n",
      "Debug cuda False\n",
      "remap_segment_embeddings None\n",
      "regularizer None\n"
     ]
    }
   ],
   "source": [
    "wordnet_batcher = KnowBertBatchifier(WORDNET_ARCHIVE)\n",
    "params = Params({\"archive_file\": WORDNET_ARCHIVE})#Only contains a dictionnary with a single entry: archive_file:http://...\n",
    "wordnet_original_model = ModelArchiveFromParams.from_params(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNetAllEmbedding(\n",
       "  (pos_embeddings): Embedding(117663, 25)\n",
       "  (entity_embeddings): Embedding(117663, 2248, padding_idx=0)\n",
       "  (proj_feed_forward): Linear(in_features=2273, out_features=200, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_original_model.soldered_kgs['wordnet'].entity_linker.disambiguator.entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kb.custom_knowbert import CustomKnowBert\n",
    "from kb.soldered_kg import CustomSolderedKG, CustomEntityLinkingWithCandidateMentions\n",
    "from kb.custom_knowledge import CustomWordNetAllEmbedding\n",
    "\n",
    "span_attention_config = {'hidden_size': 200, 'intermediate_size': 1024, 'num_attention_heads': 4, 'num_hidden_layers': 1}\n",
    "span_encoder_config = {'hidden_size': 200, 'intermediate_size': 1024, 'num_attention_heads': 4, 'num_hidden_layers': 1}\n",
    "\n",
    "#117662\n",
    "#null_entity_id = model.vocab.get_token_index('@@NULL@@', \"entity\")\n",
    "#117662\n",
    "null_entity_id = 117662\n",
    "entity_dim = 200\n",
    "\n",
    "model_entity_embedder = CustomWordNetAllEmbedding(\n",
    "                 embedding_file = WORDNET_LINKER_EMBEDDING_FILE,\n",
    "                 entity_dim = entity_dim,\n",
    "                 entity_file = WORDNET_LINKER_ENTITY_FILE,\n",
    "                 vocab_file= WORDNET_LINKER_VOCAB_FILE,\n",
    "                 entity_h5_key = \"tucker_gensen\",\n",
    "                 dropout = 0.1,\n",
    "                 pos_embedding_dim = 25,\n",
    "                 include_null_embedding = False)\n",
    "\n",
    "entity_embeddings = model_entity_embedder.entity_embeddings\n",
    "null_embedding = torch.zeros(entity_dim) #From wordnet code\n",
    "\n",
    "custom_entity_linker = CustomEntityLinkingWithCandidateMentions(\n",
    "                 null_entity_id=null_entity_id,\n",
    "                 entity_embedding = model_entity_embedder,\n",
    "                 contextual_embedding_dim =768,\n",
    "                 span_encoder_config = span_encoder_config,\n",
    "                 margin = 0.2,\n",
    "                 decode_threshold = 0.0,\n",
    "                 loss_type = 'softmax',\n",
    "                 max_sequence_length = 512,\n",
    "                 dropout = 0.1,\n",
    "                 output_feed_forward_hidden_dim = 100,\n",
    "                 initializer_range = 0.02)\n",
    "\n",
    "custom_wordnet_kg = CustomSolderedKG(entity_linker = custom_entity_linker, \n",
    "                            span_attention_config = span_attention_config,\n",
    "                            should_init_kg_to_bert_inverse = False,\n",
    "                            freeze = False)\n",
    "\n",
    "custom_soldered_kgs = {'wordnet':custom_wordnet_kg}\n",
    "\n",
    "span_extractor_global_attention_old_name = \"wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.weight\"\n",
    "span_extractor_global_attention_bias_old_name = \"wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.bias\"\n",
    "state_dict_map = {span_extractor_global_attention_old_name:span_extractor_global_attention_old_name.replace(\"._module\",\"\"),\n",
    "                span_extractor_global_attention_bias_old_name: span_extractor_global_attention_bias_old_name.replace(\"._module\",\"\")}\n",
    "\n",
    "custom_wordnet_model = CustomKnowBert(soldered_kgs = custom_soldered_kgs,\n",
    "                                soldered_layers ={\"wordnet\": 9},\n",
    "                                bert_model_name = \"bert-base-uncased\",\n",
    "                                mode=None,state_dict_file=WORDNET_MODEL_STATE_DICT_FILE,\n",
    "                                strict_load_archive=True,\n",
    "                                remap_segment_embeddings = None,\n",
    "                                state_dict_map = state_dict_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordnet entity_attention_probs are equal: True\n",
      "wordnet output linking scores are equal: True\n",
      "loss are equal : True\n",
      "pooled_output are equal : True\n",
      "pooled_output are equal : True\n",
      "contextual_embeddings are equal : True\n",
      "contextual_embeddings are equal : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "test_set = create_test_set(wordnet_original_model,wordnet_batcher)\n",
    "torch.save(test_set,'knowbert_wordnet_model_test')\n",
    "test_set = torch.load(\"knowbert_wordnet_model_test\")\n",
    "\n",
    "model_correct(custom_wordnet_model,test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiCandidateMentionGenerator params: None None True False None\n",
      "duplicate_mentions_cnt:  6777\n",
      "end of p_e_m reading. wall time: 1.357266374429067  minutes\n",
      "p_e_m_errors:  0\n",
      "incompatible_ent_ids:  0\n",
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object at 0x7f63c1e79be0> None None 0\n",
      "BertTokenizerAndCandidateGenerator params\n",
      "{'wiki': <kb.wiki_linking_util.WikiCandidateMentionGenerator object at 0x7f62c9743fd0>}\n",
      "{'wiki': <allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer object at 0x7f62c9743eb8>}\n",
      "bert-base-uncased\n",
      "True\n",
      "True\n",
      "512\n",
      "Vocab: <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "Soldered kg: {'wiki': SolderedKG(\n",
      "  (entity_linker): EntityLinkingWithCandidateMentions(\n",
      "    (loss): MarginRankingLoss()\n",
      "    (disambiguator): EntityDisambiguator(\n",
      "      (span_extractor): SelfAttentiveSpanExtractor(\n",
      "        (_global_attention): TimeDistributed(\n",
      "          (_module): Linear(in_features=300, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (bert_to_kg_projector): Linear(in_features=768, out_features=300, bias=True)\n",
      "      (projected_span_layer_norm): BertLayerNorm()\n",
      "      (kg_layer_norm): BertLayerNorm()\n",
      "      (entity_embeddings): Embedding()\n",
      "      (dot_attention_with_prior): DotAttentionWithPrior(\n",
      "        (out_layer_1): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (out_layer_2): Linear(in_features=100, out_features=1, bias=True)\n",
      "      )\n",
      "      (span_encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=300, out_features=300, bias=True)\n",
      "                (key): Linear(in_features=300, out_features=300, bias=True)\n",
      "                (value): Linear(in_features=300, out_features=300, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=300, out_features=300, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=300, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=300, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weighted_entity_layer_norm): BertLayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (span_attention_layer): SpanAttentionLayer(\n",
      "    (attention): SpanAttention(\n",
      "      (attention): SpanWordAttention(\n",
      "        (query): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (key): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (value): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=300, out_features=1024, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=300, bias=True)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer_norm): BertLayerNorm()\n",
      "  (kg_to_bert_projection): Linear(in_features=300, out_features=768, bias=True)\n",
      ")}\n",
      "Soldered layers: <allennlp.common.params.Params object at 0x7f62c96ff128>\n",
      "bert model name bert-base-uncased\n",
      "mode None\n",
      "model archive None\n",
      "strict_load_archive True\n",
      "Debug cuda False\n",
      "remap_segment_embeddings None\n",
      "regularizer None\n"
     ]
    }
   ],
   "source": [
    "wiki_batcher = KnowBertBatchifier(WIKI_ARCHIVE)\n",
    "params = Params({\"archive_file\": WIKI_ARCHIVE})\n",
    "wiki_original_model = ModelArchiveFromParams.from_params(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allennlp.modules.token_embedders.embedding.Embedding"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki_original_model.soldered_kgs['wiki'].entity_linker.disambiguator.entity_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_original_model.soldered_kgs['wiki'].entity_linker.disambiguator.entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kb.custom_tokenizer.vocabulary import Vocabulary\n",
    "vocabulary = Vocabulary.from_files(\"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/wiki_entity_linking/vocabulary_wiki.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"Hello I am good\"\n",
    "\n",
    "line.split(\" \",1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pretrained embeddings from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "b'A_Forest 0.043461049344163 -0.083031912273826 0.015892639209858 -0.019632200966898 -0.030101784401457 0.037633704590477 -0.072133985662249 -0.10074446004005 0.069966410675443 -0.049433579831659 -0.039654540397471 -0.03242789842469 0.018648329481155 -0.052262736550407 -0.061408946042189 -0.070215945512216 0.052235180579016 -0.0016973801537844 -0.05759637604839 -0.015419615833859 0.03305185472591 0.11210366698168 -0.10238323753775 -0.021602179112486 -0.016465255424595 0.079950300090811 -0.093411107130336 -0.0092436798464461 0.054513717058838 0.055219987371663 -0.030644296546327 0.072715689722356 -0.0385469781022 -0.068647345030748 -0.022262602279184 0.080115352797101 -0.021307244164405 -0.022041506307571 0.014280509644071 0.041370047696809 0.026127087916798 -0.023551068390419 -0.051852362310526 0.054481135671007 0.07898001101303 0.012865536403939 -0.013461836150934 -0.062267074083521 0.023079185884535 0.049899390114566 0.062010714515155 0.0089464231112669 0.013683554246006 -0.048700014317756 0.081883680985765 -0.053590027635798 0.04340524449741 0.0075701691401825 0.0011409652259987 -0.0031748622495762 0.0012123490034132 -0.0220071591322 -0.010095550331614 -0.10402381062531 -0.064800953127492 0.034357207577488 0.0088262443190473 0.071745639063145 -0.017012088906062 0.079453696559356 -0.074779396168631 -0.10464945820826 -0.01756230308836 0.050196008893803 0.037663484560099 0.077506211715817 -0.019047125519186 0.094773795922926 0.016794705186104 0.025083317490405 -0.040368313444699 -0.025687827776936 -0.01724136934018 -0.018510286991246 0.026254412746886 -0.08270712657561 -0.048259990493409 -0.02014612896029 0.024516322738535 0.05751832376874 0.052409241036938 0.044699343247383 0.098384227946789 -0.12022995535475 -0.019558090044248 -0.057764543097262 0.052298160334637 0.034828788334868 -0.06584921997912 0.026249316549933 0.048403175746398 0.03713990992781 -0.092019256766292 0.038880108450398 -0.044574985610915 -0.046340139852354 -0.026177081310884 -0.093173873201705 -0.072052066531402 -0.079463345060898 0.004860319495755 0.1109657920003 -0.06843708220295 -0.03790902588581 -0.010626994639387 0.0083378652969661 0.0091525192020088 0.045123910842813 0.042190196404853 0.044806307504045 0.021014782808998 -0.049967067461091 -0.057795015970856 0.13891461014035 0.031448791585075 -0.061855764795807 0.07983528548209 -0.10643837235041 0.038919216546607 -0.048922872351619 0.020738500388801 -0.076679376111917 -0.032541944457967 0.017852370944832 0.042002747254059 -0.0034175432509285 -0.070302439299393 0.0017247617350287 0.074239050279995 -0.036857726647983 0.07361213609838 0.052620248922769 0.0067720895167691 0.034632908852419 -0.036237253493072 -0.0060710085331513 0.060043042324182 -0.06129768280067 0.021045412144779 0.065413167314095 -0.053453819851297 0.024876021856228 0.075888936592982 0.021919687456354 0.033659579937858 -0.015655220879364 -0.028619863971674 -0.098102543855845 -0.057126810672992 0.02896149543204 -0.059456920069933 -0.089724440768426 0.016460745960844 0.094388638172209 -0.04520693638484 0.035249410848008 -0.027112194336232 -0.048552060693348 -0.062341594788089 -0.037600504804478 0.045820331488427 -0.054536407801266 0.030673504683908 -0.06186786081299 -0.065331374843113 0.067075719613661 -0.047068430355376 -0.03164435069257 0.0070972915161616 0.12139603078272 0.059442261053112 -0.032755627102141 -0.0046459984299781 0.11406020428034 -0.03464452430717 0.092158962598262 -0.067919736255827 0.040779496072547 0.038783135421972 0.045823565040295 -0.0078227629058475 -0.075848181918518 0.0044477301052067 -0.12000674341832 -0.066131515017652 0.031791998843184 0.095485758479132 0.023613604836509 -0.041861420920865 -0.030009698953735 0.051026033346089 0.043058665851697 -0.060485547191783 0.086844433838853 0.11079372829791 0.0026002548672434 -0.078824092718238 0.15280176173872 -0.05287276771697 0.13433603480585 0.093203913941639 0.075582211101512 0.015975186052099 0.0060072525201913 -0.013055750651362 -0.092639569733726 0.072831531345495 -0.088723365892676 -0.034925414911308 -0.097507033870366 0.042880000935125 -0.057364968473586 -0.030701451810767 -0.06176794108004 0.01707259134372 -0.015404027357141 -0.032681907334959 -0.041817168198931 -0.044447621665279 -0.028194392993377 -0.010487570066825 -0.035710121208671 0.12021676037696 0.041781163269434 -0.039829804124117 -0.03174250091269 0.013551148120125 -0.031063186799537 -0.02417111313696 -0.077326019430275 0.022345815948365 0.078928676514482 -0.037255867034944 -0.06751885033001 0.09212130736522 0.046805436045217 0.017841599268305 0.05977954137456 0.040266769485234 0.018199912578054 -0.086926382772021 0.093419749803531 -0.030526959219185 0.007404445416001 -0.050237281383601 -0.0053762307405018 -0.035901973652446 -0.015913849149443 0.100130442813 -0.013096403811669 -0.059733314248835 -0.027220186772981 0.035302887388404 0.043757500485342 -0.02929304066927 0.050705948964068 -0.049342179837329 -0.01315636887622 -0.019134990213145 -0.0048799354768085 0.12922951134839 -0.097057436050281 0.064953026922828 -0.026635092698819 0.0016092673787832 0.025665800136158 -0.049715919573674 -0.06046210021545 0.056910956184649 -0.045637531499766 0.019056597069444 0.044222718447112 -0.19224155778432 0.08637826592809 0.058235118026297 -0.064592299625048 -0.035749497525772 -0.13170329773662 0.030646727298163 0.034684552550051 -0.049871457888868 -0.019130998564728 0.070087497507141 -0.0024485172793453 -0.0041474787023917 -0.054052969445273 0.08255933686397 0.053889775658778 -0.015127690920237 -0.038118726194792\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bbd70e6f56d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# shutil.rmtree(tempdir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mread_embeddings_from_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-bbd70e6f56d7>\u001b[0m in \u001b[0;36mread_embeddings_from_text_file\u001b[0;34m(gzip_filename, embedding_dim, vocab, namespace)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_to_keep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def read_embeddings_from_text_file(gzip_filename: str,\n",
    "                                    embedding_dim: int,\n",
    "                                    vocab: Vocabulary,\n",
    "                                    namespace: str = \"tokens\") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Read pre-trained word vectors from an eventually compressed text file, possibly contained\n",
    "    inside an archive with multiple files. The text file is assumed to be utf-8 encoded with\n",
    "    space-separated fields: [word] [dim 1] [dim 2] ...\n",
    "\n",
    "    Lines that contain more numerical tokens than ``embedding_dim`` raise a warning and are skipped.\n",
    "\n",
    "    The remainder of the docstring is identical to ``_read_pretrained_embeddings_file``.\n",
    "    \"\"\"\n",
    "    tokens_to_keep = set(vocab.get_index_to_token_vocabulary(namespace).values())\n",
    "    vocab_size = vocab.get_vocab_size(namespace)\n",
    "    embeddings = {}\n",
    "\n",
    "    # First we read the embeddings from the file, only keeping vectors for the words we need.\n",
    "    print(\"Reading pretrained embeddings from file\")\n",
    "\n",
    "    with open(gzip_filename) as embeddings_file:\n",
    "        for line in tqdm(embeddings_file):\n",
    "            print(type(line))\n",
    "            print(line)\n",
    "            token = line.split(' ', 1)[0]\n",
    "            if token in tokens_to_keep:\n",
    "                fields = line.rstrip().split(' ')\n",
    "                if len(fields) - 1 != embedding_dim:\n",
    "                    # Sometimes there are funny unicode parsing problems that lead to different\n",
    "                    # fields lengths (e.g., a word with a unicode space character that splits\n",
    "                    # into more than one column).  We skip those lines.  Note that if you have\n",
    "                    # some kind of long header, this could result in all of your lines getting\n",
    "                    # skipped.  It's hard to check for that here; you just have to look in the\n",
    "                    # embedding_misses_file and at the model summary to make sure things look\n",
    "                    # like they are supposed to.\n",
    "                    print(\"Found line with wrong number of dimensions (expected: %d; actual: %d): %s\",\n",
    "                                   embedding_dim, len(fields) - 1, line)\n",
    "                    continue\n",
    "\n",
    "                vector = np.asarray(fields[1:], dtype='float32')\n",
    "                embeddings[token] = vector\n",
    "\n",
    "    if not embeddings:\n",
    "        raise Exception(\"No embeddings of correct dimension found; you probably \"\n",
    "                                 \"misspecified your embedding_dim parameter, or didn't \"\n",
    "                                 \"pre-populate your Vocabulary\")\n",
    "\n",
    "    all_embeddings = np.asarray(list(embeddings.values()))\n",
    "    embeddings_mean = float(np.mean(all_embeddings))\n",
    "    embeddings_std = float(np.std(all_embeddings))\n",
    "    # Now we initialize the weight matrix for an embedding layer, starting with random vectors,\n",
    "    # then filling in the word vectors we just read.\n",
    "    print(\"Initializing pre-trained embedding layer\")\n",
    "    embedding_matrix = torch.FloatTensor(vocab_size, embedding_dim).normal_(embeddings_mean,\n",
    "                                                                            embeddings_std)\n",
    "    num_tokens_found = 0\n",
    "    index_to_token = vocab.get_index_to_token_vocabulary(namespace)\n",
    "    for i in range(vocab_size):\n",
    "        token = index_to_token[i]\n",
    "\n",
    "        # If we don't have a pre-trained vector for this word, we'll just leave this row alone,\n",
    "        # so the word has a random initialization.\n",
    "        if token in embeddings:\n",
    "            embedding_matrix[i] = torch.FloatTensor(embeddings[token])\n",
    "            num_tokens_found += 1\n",
    "        else:\n",
    "            print(\"Token %s was not found in the embedding file. Initialising randomly.\", token)\n",
    "\n",
    "    print(\"Pretrained embeddings were found for %d out of %d tokens\",\n",
    "                num_tokens_found, vocab_size)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "compressed_embedding_file = cached_path(\"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/wiki_entity_linking/entities_glove_format.gz\")\n",
    "embedding_file = compressed_embedding_file+'+unzipped'\n",
    "with gzip.open(compressed_embedding_file, 'rb') as f_in:\n",
    "    with open(embedding_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "with open(embedding_file) as f:\n",
    "    print(f.readline())\n",
    "# print(embedding_file)\n",
    "# tempdir = embedding_file+\"_extracted\"\n",
    "\n",
    "# print(zipfile.is_zipfile(embedding_file))\n",
    "# print(tarfile.is_tarfile(embedding_file))\n",
    "\n",
    "# with gzip.open(embedding_file) as f:\n",
    "#     print(f.readline())\n",
    "\n",
    "# os.makedirs(tempdir,exist_ok=True)\n",
    "# print(\"Extracting model archives\")\n",
    "# with tarfile.open(embedding_file, 'r:gz') as archive:\n",
    "#     archive.extractall(tempdir)\n",
    "\n",
    "#Remove temporary directory\n",
    "# shutil.rmtree(tempdir)\n",
    "\n",
    "read_embeddings_from_text_file(embedding_file,embedding_dim=300,vocab=vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model archives\n"
     ]
    }
   ],
   "source": [
    "from kb.custom_knowbert import CustomKnowBert\n",
    "from kb.soldered_kg import CustomSolderedKG, CustomEntityLinkingWithCandidateMentions\n",
    "from kb.custom_knowledge import CustomWordNetAllEmbedding\n",
    "from allennlp.common.file_utils import cached_path\n",
    "import torch\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "wiki_state_dict_file = cached_path(WIKI_ARCHIVE)\n",
    "\n",
    "# span_attention_config = {'hidden_size': 200, 'intermediate_size': 1024, 'num_attention_heads': 4, 'num_hidden_layers': 1}\n",
    "\n",
    "\n",
    "#null_entity_id = model.vocab.get_token_index('@@NULL@@', \"entity\")\n",
    "\n",
    "entity_dim = 300\n",
    "\n",
    "\n",
    "custom_embedding = wiki_original_model.soldered_kgs['wiki'].entity_linker.disambiguator.entity_embeddings\n",
    "null_entity_id = vocabulary.get_token_index('@@NULL@@', \"entity\")\n",
    "span_encoder_config = {'hidden_size': 300, 'intermediate_size': 1024, 'num_attention_heads': 4, 'num_hidden_layers': 1}\n",
    "\n",
    "custom_entity_linker = CustomEntityLinkingWithCandidateMentions(\n",
    "                 null_entity_id=null_entity_id,\n",
    "                 entity_embedding = custom_embedding,\n",
    "                 contextual_embedding_dim =768,\n",
    "                 span_encoder_config = span_encoder_config,\n",
    "                 margin = 0.2,\n",
    "                 decode_threshold = 0.0,\n",
    "                 loss_type = 'softmax',\n",
    "                 max_sequence_length = 512,\n",
    "                 dropout = 0.1,\n",
    "                 output_feed_forward_hidden_dim = 100,\n",
    "                 initializer_range = 0.02)\n",
    "\n",
    "custom_entity_linker = wiki_original_model.soldered_kgs['wiki'].entity_linker\n",
    "span_attention_config = {'hidden_size': 300, 'intermediate_size': 1024, 'num_attention_heads': 4, 'num_hidden_layers': 1}\n",
    "\n",
    "custom_wiki_kg = CustomSolderedKG(entity_linker = custom_entity_linker, \n",
    "                            span_attention_config = span_attention_config,\n",
    "                            should_init_kg_to_bert_inverse = False,\n",
    "                            freeze = False)\n",
    "\n",
    "custom_soldered_kgs = {'wiki':custom_wiki_kg}\n",
    "#custom_soldered_kgs = {'wiki':wiki_original_model.soldered_kgs['wiki']}\n",
    "\n",
    "span_extractor_global_attention_old_name = \"wiki_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.weight\"\n",
    "span_extractor_global_attention_bias_old_name = \"wiki_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.bias\"\n",
    "state_dict_map = {span_extractor_global_attention_old_name:span_extractor_global_attention_old_name.replace(\"._module\",\"\"),\n",
    "                span_extractor_global_attention_bias_old_name: span_extractor_global_attention_bias_old_name.replace(\"._module\",\"\")}\n",
    "\n",
    "#Temporary, for using custom_soldered_k\n",
    "state_dict_map = None\n",
    "\n",
    "tempdir = wiki_state_dict_file+\"_extracted\"\n",
    "os.makedirs(tempdir,exist_ok=True)\n",
    "\n",
    "print(\"Extracting model archives\")\n",
    "with tarfile.open(wiki_state_dict_file, 'r:gz') as archive:\n",
    "    archive.extractall(tempdir)\n",
    "\n",
    "weights_file = tempdir+'/weights.th'\n",
    "\n",
    "\n",
    "custom_wiki_model = CustomKnowBert(soldered_kgs = custom_soldered_kgs,\n",
    "                                soldered_layers ={\"wiki\": 9},\n",
    "                                bert_model_name = \"bert-base-uncased\",\n",
    "                                mode=None,state_dict_file=weights_file,\n",
    "                                strict_load_archive=True,\n",
    "                                remap_segment_embeddings = None,\n",
    "                                state_dict_map = state_dict_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki entity_attention_probs are equal: True\n",
      "wiki output linking scores are equal: True\n",
      "loss are equal : True\n",
      "pooled_output are equal : True\n",
      "pooled_output are equal : True\n",
      "contextual_embeddings are equal : True\n",
      "contextual_embeddings are equal : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "test_set = create_test_set(wiki_original_model,wiki_batcher)\n",
    "torch.save(test_set,'knowbert_wiki_model_test')\n",
    "test_set = torch.load(\"knowbert_wiki_model_test\")\n",
    "\n",
    "custom_model = custom_wiki_model\n",
    "model_correct(custom_model,test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equal(original_model,custom_model,batches):\n",
    "\n",
    "    for (name_model,param_model), (name_custom_model,param_custom_model) in zip(original_model.soldered_kgs['wordnet'].named_parameters(),custom_model.soldered_kgs['wordnet'].named_parameters()):\n",
    "        # if(name_model!=name_custom_model):\n",
    "        #     print(f\"model_name:{name_model} is not equal to custom_model_name:{name_custom_model}\")\n",
    "        if(not torch.equal(param_model,param_custom_model)):\n",
    "            print(f\"Tensor values are not equal for custom_param:{name_custom_model} and model_param{name_model}\")\n",
    "\n",
    "    original_model.eval()\n",
    "    custom_model.eval()\n",
    "    # batcher takes raw untokenized sentences\n",
    "    # and yields batches of tensors needed to run KnowBert\n",
    "    for batch in batches:\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**batch)\n",
    "            custom_output = custom_model(**batch)\n",
    "\n",
    "        for key in original_output.keys():\n",
    "            if(key in ['wiki','wordnet']):\n",
    "                print(f\"wordnet entity_attention_probs are equal: {torch.equal(original_output[key]['entity_attention_probs'],custom_output[key]['entity_attention_probs'])}\")\n",
    "\n",
    "                print(f\"Output linking scores are equal: {torch.equal(original_output[key]['linking_scores'],custom_output[key]['linking_scores'])}\")\n",
    "            else:\n",
    "                print(f\"Loss are equal : {original_output['loss']==custom_output['loss']}\")\n",
    "\n",
    "                print(f\"Pooled outputs are equal : {torch.equal(original_output['pooled_output'],custom_output['pooled_output'])}\")\n",
    "\n",
    "                print(f\"Contextual embeddings are equal : {torch.equal(original_output['contextual_embeddings'],custom_output['contextual_embeddings'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4f27553e8766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcustom_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpected_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected_output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/knowbert/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_batch'"
     ]
    }
   ],
   "source": [
    "batch = torch.load(\"test_batch\")\n",
    "custom_output = custom_model(**batch)\n",
    "expected_output = torch.load(\"expected_output\")\n",
    "\n",
    "\n",
    "print(f\"wordnet entity_attention_probs are equal: {torch.equal(expected_output['wordnet']['entity_attention_probs'],custom_output['wordnet']['entity_attention_probs'])}\")\n",
    "\n",
    "print(f\"Output linking scores are equal: {torch.equal(expected_output['wordnet']['linking_scores'],custom_output['wordnet']['linking_scores'])}\")\n",
    "\n",
    "print(f\"Loss are equal : {expected_output['loss']==custom_output['loss']}\")\n",
    "\n",
    "print(f\"Pooled outputs are equal : {torch.equal(expected_output['pooled_output'],custom_output['pooled_output'])}\")\n",
    "\n",
    "print(f\"Contextual embeddings are equal : {torch.equal(expected_output['contextual_embeddings'],custom_output['contextual_embeddings'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is located in France.\n",
      "['[CLS]', 'paris', 'is', 'located', 'in', 'france', '.', '[SEP]']\n",
      "Michael Jackson is a great music singer\n",
      "['[CLS]', 'michael', 'jackson', 'is', 'a', 'great', 'music', 'singer', '[SEP]']\n",
      "\n",
      "Input\n",
      "\n",
      "Batch: dict_keys(['tokens', 'segment_ids', 'candidates'])\n",
      "Tokens shape torch.Size([2, 9])\n",
      "Segment ids shape: torch.Size([2, 9])\n",
      "Wordnet kb: dict_keys(['candidate_entity_priors', 'candidate_entities', 'candidate_spans', 'candidate_segment_ids'])\n",
      "Candidate entity_priors shape: torch.Size([2, 8, 14])\n",
      "Candidate entities ids shape: torch.Size([2, 8, 14])\n",
      "Candidate span shape: torch.Size([2, 8, 2])\n",
      "Candidate segments_ids shape: torch.Size([2, 8])\n",
      "\n",
      "Output\n",
      "\n",
      "Model output keys: dict_keys(['wordnet', 'loss', 'pooled_output', 'contextual_embeddings'])\n",
      "Output wordnet keys: dict_keys(['entity_attention_probs', 'linking_scores'])\n",
      "Output wordnet entity_attention_probs shape: torch.Size([2, 4, 9, 8])\n",
      "Output wordnet linking_scores shape: torch.Size([2, 8, 14])\n",
      "Output loss: 0.0\n",
      "Pooled output shape: torch.Size([2, 768])\n",
      "Contextual embeddings: torch.Size([2, 9, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Paris is located in France.\", \"Michael Jackson is a great music singer\"]\n",
    "# batcher takes raw untokenized sentences\n",
    "# and yields batches of tensors needed to run KnowBert\n",
    "for i,batch in enumerate(batcher.iter_batches(sentences, verbose=True)):\n",
    "\n",
    "    print(f\"\\nInput\\n\")\n",
    "    print(f\"Batch: {batch.keys()}\") #Batch contains {tokens,segment_ids,candidates}\n",
    "    #tokens: Tensor of tokens indices (used to idx an embedding) => because a batch contains multiple\n",
    "    #sentences with varying # of tokens, all tokens tensors are padded with zeros \n",
    "    #shape: (batch_size (#sentences), max_seq_len)\n",
    "    #print(batch['tokens'])#dict with only 'tokens'\n",
    "    print(f\"Tokens shape {batch['tokens']['tokens'].shape}\")\n",
    "    #Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "    #shape: (batch_size,max_seq_len)\n",
    "    print(f\"Segment ids shape: {batch['segment_ids'].shape}\")\n",
    "\n",
    "    #Dict with only wordnet\n",
    "    #Candidates: stores for multiple knowledge base, the entities detected using this knowledge base\n",
    "    wordnet_kb = batch['candidates']['wordnet']\n",
    "    print(f\"Wordnet kb: {wordnet_kb.keys()}\")\n",
    "    \n",
    "    #Stores for each detected entities, a list of candidate KB entities that correspond to it\n",
    "    #Priors: correctness probabilities estimated by the entity linker (sum to 1 (or 0 if padding) on axis 2)\n",
    "    #Adds 0 padding to axis 1 when there is less detected entities in the sentence than in the max sentence\n",
    "    #Adds 0 padding to axis 2 when there is less detected KB entities for an entity in the sentence than in the max candidate KB entities entity\n",
    "    #shape:(batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entity_priors shape: {wordnet_kb['candidate_entity_priors'].shape}\")\n",
    "    #Ids of the KB candidate entities + 0 padding on axis 1 or 2 if necessary\n",
    "    #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entities ids shape: {wordnet_kb['candidate_entities']['ids'].shape}\")\n",
    "    #Spans of which sequence of tokens correspond to an entity in the sentence, eg: [1,2] for Michael Jackson (both bounds are included)\n",
    "    #Padding with [-1,-1] when no more detected entities\n",
    "    #shape: (batch_size, max # detected entities, 2)\n",
    "    print(f\"Candidate span shape: {wordnet_kb['candidate_spans'].shape}\")\n",
    "\n",
    "    #For each sentence entity, indicate to which segment ids it corresponds to\n",
    "    #shape: (batch_size, max # detected entities)\n",
    "    print(f\"Candidate segments_ids shape: {wordnet_kb['candidate_segment_ids'].shape}\")\n",
    "\n",
    "    #model(**batch) <=> model(tokens = batch['tokens'],segment_ids=batch['segment_ids'],candidates=batch['candidates']) \n",
    "    model_output = model(**batch)\n",
    "    \n",
    "    print(f\"\\nOutput\\n\")\n",
    "    print(f\"Model output keys: {model_output.keys()}\")\n",
    "    print(f\"Output wordnet keys: {model_output['wordnet'].keys()}\")\n",
    "    #Span attention layers scores for wordnet KB\n",
    "    #shape: (batch_size,?,max_seq_len,max # detected entities)\n",
    "    print(f\"Output wordnet entity_attention_probs shape: {model_output['wordnet']['entity_attention_probs'].shape}\")\n",
    "    #Entity linker score for each text entity and possible KB entity, -1.0000e+04 padding in case of no score\n",
    "    #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Output wordnet linking_scores shape: {model_output['wordnet']['linking_scores'].shape}\")\n",
    "    \n",
    "    #Scalar indicating loss over this batch (0 if not training?)\n",
    "    print(f\"Output loss: {model_output['loss']}\")\n",
    "\n",
    "    #Final CLS embedding for each sentence of batch\n",
    "    # shape: (batch_size, hidden_size) \n",
    "    print(f\"Pooled output shape: {model_output['pooled_output'].shape}\")\n",
    "\n",
    "    #For each tokens, its final embeddings\n",
    "    #Important!!!, still predicts something for 0 padded tokens => ignore (or 0 padding <=> MASK???)\n",
    "    print(f\"Contextual embeddings: {model_output['contextual_embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #TODO: see how to add masking => 0 idx tokens embedding?\n",
    "    #TODO: See how to extract from final embeddings the actual predicted tokens\n",
    "    #TODO: copy paste all allennlp dependencies in an allennlp.py file that contains all classes => get rid of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_bert.bert.embeddings.word_embeddings.weight:torch.Size([30522, 768])\n",
      "pretrained_bert.bert.embeddings.position_embeddings.weight:torch.Size([512, 768])\n",
      "pretrained_bert.bert.embeddings.token_type_embeddings.weight:torch.Size([2, 768])\n",
      "pretrained_bert.bert.embeddings.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.embeddings.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.0.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.0.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.0.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.0.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.1.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.1.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.1.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.1.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.2.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.2.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.2.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.2.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.3.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.3.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.3.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.3.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.4.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.4.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.4.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.4.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.5.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.5.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.5.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.5.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.6.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.6.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.6.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.6.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.7.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.7.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.7.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.7.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.8.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.8.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.8.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.8.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.9.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.9.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.9.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.9.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.10.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.10.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.10.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.10.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.query.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.query.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.key.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.key.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.value.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.self.value.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.intermediate.dense.weight:torch.Size([3072, 768])\n",
      "pretrained_bert.bert.encoder.layer.11.intermediate.dense.bias:torch.Size([3072])\n",
      "pretrained_bert.bert.encoder.layer.11.output.dense.weight:torch.Size([768, 3072])\n",
      "pretrained_bert.bert.encoder.layer.11.output.dense.bias:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.output.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.bert.encoder.layer.11.output.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.bert.pooler.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.bert.pooler.dense.bias:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.bias:torch.Size([30522])\n",
      "pretrained_bert.cls.predictions.transform.dense.weight:torch.Size([768, 768])\n",
      "pretrained_bert.cls.predictions.transform.dense.bias:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.transform.LayerNorm.weight:torch.Size([768])\n",
      "pretrained_bert.cls.predictions.transform.LayerNorm.bias:torch.Size([768])\n",
      "pretrained_bert.cls.seq_relationship.weight:torch.Size([2, 768])\n",
      "pretrained_bert.cls.seq_relationship.bias:torch.Size([2])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.weight:torch.Size([1, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_extractor._global_attention._module.bias:torch.Size([1])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.bert_to_kg_projector.weight:torch.Size([200, 768])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.bert_to_kg_projector.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.projected_span_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.projected_span_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.kg_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.kg_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.pos_embeddings.weight:torch.Size([117663, 25])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.entity_embeddings.weight:torch.Size([117663, 2248])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.proj_feed_forward.weight:torch.Size([200, 2273])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.entity_embeddings.proj_feed_forward.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_1.weight:torch.Size([100, 2])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_1.bias:torch.Size([100])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_2.weight:torch.Size([1, 100])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.dot_attention_with_prior.out_layer_2.bias:torch.Size([1])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.query.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.query.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.key.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.key.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.value.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.self.value.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.dense.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.attention.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.intermediate.dense.weight:torch.Size([1024, 200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.intermediate.dense.bias:torch.Size([1024])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.dense.weight:torch.Size([200, 1024])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.entity_linker.disambiguator.span_encoder.layer.0.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.weighted_entity_layer_norm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.weighted_entity_layer_norm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.query.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.query.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.key.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.key.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.value.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.attention.value.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.dense.weight:torch.Size([200, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.attention.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.intermediate.dense.weight:torch.Size([1024, 200])\n",
      "wordnet_soldered_kg.span_attention_layer.intermediate.dense.bias:torch.Size([1024])\n",
      "wordnet_soldered_kg.span_attention_layer.output.dense.weight:torch.Size([200, 1024])\n",
      "wordnet_soldered_kg.span_attention_layer.output.dense.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.output.LayerNorm.weight:torch.Size([200])\n",
      "wordnet_soldered_kg.span_attention_layer.output.LayerNorm.bias:torch.Size([200])\n",
      "wordnet_soldered_kg.output_layer_norm.weight:torch.Size([768])\n",
      "wordnet_soldered_kg.output_layer_norm.bias:torch.Size([768])\n",
      "wordnet_soldered_kg.kg_to_bert_projection.weight:torch.Size([768, 200])\n",
      "wordnet_soldered_kg.kg_to_bert_projection.bias:torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}:{param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9797c430b9b8ca5d4cad34220fe2c597e42ba7691ef10261f4554305aef3ef0a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('knowbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
