{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTokenizer params None False None None\n",
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object at 0x7f78ab53cb38> None None 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/root/.conda/envs/knowbert/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from kb.knowbert_utils import KnowBertBatchifier\n",
    "\n",
    "\n",
    "\n",
    "WORDNET_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wordnet_model.tar.gz\"\n",
    "WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_model.tar.gz\"\n",
    "WORDNET_WIKI_ARCHIVE = \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_wordnet_model.tar.gz\"\n",
    "\n",
    "WORDNET_FOLDER = '../knowbert_wordnet_model/'\n",
    "WORDNET_LINKER_FOLDER = WORDNET_FOLDER + 'entity_linker/'\n",
    "WORDNET_LINKER_EMBEDDING_FILE = WORDNET_LINKER_FOLDER + 'wordnet_synsets_mask_null_vocab_embeddings_tucker_gensen.hdf5'\n",
    "WORDNET_LINKER_ENTITY_FILE = WORDNET_LINKER_FOLDER + 'entities.jsonl'\n",
    "WORDNET_LINKER_VOCAB_FILE = WORDNET_LINKER_FOLDER + 'wordnet_synsets_mask_null_vocab.txt'\n",
    "\n",
    "\n",
    "WORDNET_MODEL_STATE_DICT_FILE = WORDNET_FOLDER+ 'weights.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiCandidateMentionGenerator params: None None True False None\n",
      "duplicate_mentions_cnt:  6777\n",
      "end of p_e_m reading. wall time: 1.2168683171272279  minutes\n",
      "p_e_m_errors:  0\n",
      "incompatible_ent_ids:  0\n",
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer object at 0x7f78ab53cb38> None None 0\n",
      "BertTokenizerAndCandidateGenerator params\n",
      "{'wiki': <kb.wiki_linking_util.WikiCandidateMentionGenerator object at 0x7f7b3248e3c8>}\n",
      "{'wiki': <allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer object at 0x7f7b2e715390>}\n",
      "bert-base-uncased\n",
      "True\n",
      "True\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "original_batcher = KnowBertBatchifier(WIKI_ARCHIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kb.custom_tokenizer.custom_tokenizer import CustomKnowBertBatchifier\n",
    "from kb.custom_tokenizer.bert_tokenizer_and_candidate_generator import TokenizerAndCandidateGenerator, BertTokenizerAndCandidateGenerator\n",
    "from kb.custom_tokenizer.wiki_linking_util import WikiCandidateMentionGenerator\n",
    "#from kb.bert_tokenizer_and_candidate_generator import TokenizerAndCandidateGenerator, BertTokenizerAndCandidateGenerator\n",
    "#from kb.wiki_linking_util import WikiCandidateMentionGenerator\n",
    "from allennlp.data import Vocabulary\n",
    "\n",
    "from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "from allennlp.common import Params\n",
    "\n",
    "\n",
    "candidate_generator_params = {\n",
    "                        \"type\": \"bert_tokenizer_and_candidate_generator\",\n",
    "                        \"bert_model_type\": \"bert-base-uncased\",\n",
    "                        \"do_lower_case\": True,\n",
    "                        \"entity_candidate_generators\": {\n",
    "                            \"wiki\": {\n",
    "                                \"type\": \"wiki\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"entity_indexers\": {\n",
    "                            \"wiki\": {\n",
    "                                \"type\": \"characters_tokenizer\",\n",
    "                                \"namespace\": \"entity\",\n",
    "                                \"tokenizer\": {\n",
    "                                    \"type\": \"word\",\n",
    "                                    \"word_splitter\": {\n",
    "                                        \"type\": \"just_spaces\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiCandidateMentionGenerator params: None None True False None\n",
      "duplicate_mentions_cnt:  6777\n",
      "end of p_e_m reading. wall time: 1.330450212955475  minutes\n",
      "p_e_m_errors:  0\n",
      "incompatible_ent_ids:  0\n",
      "TokenCharactersIndexer params: entity <allennlp.data.tokenizers.word_tokenizer.WordTokenizer object at 0x7f74d0cab2e8> None None 0\n"
     ]
    }
   ],
   "source": [
    "custom_candidate_mention_generator = WikiCandidateMentionGenerator()#use default params: None None True False None\n",
    "entity_candidate_generators = {'wiki':custom_candidate_mention_generator}\n",
    "\n",
    "character_tokenizer = WordTokenizer(JustSpacesWordSplitter())#use default params for the rest\n",
    "custom_entity_indexer = TokenCharactersIndexer('entity',character_tokenizer) #use default params for rest\n",
    "entity_indexers = {'wiki':custom_entity_indexer}\n",
    "\n",
    "bert_model_type = 'bert-base-uncased'\n",
    "custom_tokenizer_and_candidate_generator = BertTokenizerAndCandidateGenerator(entity_candidate_generators,entity_indexers,bert_model_type,do_lower_case=True,whitespace_tokenize=True,max_word_piece_sequence_length=512) \n",
    "\n",
    "vocabulary = Vocabulary.from_params(Params({\"directory_path\": \"https://allennlp.s3-us-west-2.amazonaws.com/knowbert/wiki_entity_linking/vocabulary_wiki.tar.gz\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris_Hilton'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.get_token_index(\"Paris_Hilton\")\n",
    "vocabulary.get_token_from_index(1,'entity')\n",
    "vocabulary.get_token_from_index(156993,'entity')\n",
    "# vocabulary._index_to_token['entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_batcher = CustomKnowBertBatchifier(custom_tokenizer_and_candidate_generator,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['wiki'])\n",
      "[['Paris', 'Paris_(mythology)', 'Paris_Hilton', 'Paris,_Texas', 'Paris,_Kentucky', 'Paris_Las_Vegas', 'Paris_Masters', 'Paris_(Paris_Hilton_album)', 'Paris,_Missouri', 'Paris_Metropolitan_Area', 'Paris_(The_Cure_album)', 'Count_Paris', 'University_of_Paris', 'Paris_(rapper)', 'Paris,_Ontario', 'Paris,_Maine', 'Paris_(Malcolm_McLaren_album)', 'Paris,_Kenosha_County,_Wisconsin', 'Paris,_Grant_County,_Wisconsin', 'Liberation_of_Paris', 'Bubba_Paris', 'Paris,_Illinois', 'Paris,_Virginia', 'Charles_de_Gaulle_Airport', 'Paris_FC', 'Live_8_concert,_Paris', 'Open_GDF_Suez', 'Paris_Métro', 'Paris,_Arkansas', 'Paris,_Tennessee'], ['Locus_(genetics)', 'Space_Shuttle_Challenger_disaster', 'South_Padre_Island', 'Location_(geography)', 'Map_Room_(White_House)', 'Sderot', \"Shenzhen_Bao'an_International_Airport\", 'Rural_Khmer_house', 'Liver', 'Search_for_HMAS_Sydney_and_German_auxiliary_cruiser_Kormoran', 'Nazi_concentration_camps', 'Yellow_Sea', 'Battle_between_HMAS_Sydney_and_German_auxiliary_cruiser_Kormoran', 'Tallapoosa_County,_Alabama', 'Naypyidaw', 'Topkapı_Palace', 'Latvia', 'Drax_power_station', 'Palazzo_Malta', 'World_Geodetic_System', 'CBLFT-DT'], ['Full_stop', 'Names_of_Burma', 'University_of_Yangon', 'Ne_Win', 'Administrative_divisions_of_Burma', 'Wa_people', 'Yangon_River', 'U_Nu', 'Yangon_Technological_University', 'U_Thant', 'Yangon', 'Naypyidaw', 'Greater_Tokyo_Area', 'Allies_of_World_War_II', 'Shiva', '1991_Bangladesh_cyclone', 'Bhopal', 'Ba_Win', 'Doctor_of_Philosophy', 'Walter_Chit_Tun', 'Big_Brother_(UK)', 'Harlem', 'Combining_character', '8888_Uprising', 'Union_Solidarity_and_Development_Association', 'Uncanny', 'Yangon_Region', 'Derby', 'A_Coruña', 'Shwe_Mann'], ['Michael_(archangel)', 'Michael', 'Michael_Jackson', 'Michael_(album)', 'Michael_Scott_(The_Office)', 'Michael_Dawson_(Lost)', 'Michael_(1996_film)', 'Michael_I_of_Romania', 'Michael_Corleone', 'Michael_Scofield', 'Michael_Jordan', 'Guinness', 'Michael_Arrington', 'Michael_Schumacher', 'Michael_Eisner', 'Michael_Bay', 'Michael,_Isle_of_Man', 'Michael_(Franz_Ferdinand_song)', 'Michael_J._Nelson', 'M._Blane_Michael', 'Michael_Giacchino', 'Michael_Andretti', 'Michael_DeLuise', 'Michael_(1924_film)', 'Michael_Schenker', 'Michael_Corinthos', 'Michael_(ship)', 'Michael_Maltese', 'Michael_Hudson_(economist)', 'Michael_Palin'], ['Great_Comet', 'Megalosaurus', 'Great_Island,_Falkland_Islands', 'The_Great_Masturbator', '1938_New_England_hurricane', 'Great_Depression', 'Great_River_(Jamaica)', 'Haole', 'Great_Russia', 'Great_Dane', 'Great_Barrington,_Massachusetts', 'Great_Shearwater', 'Great_(supermarket)', 'Great_Cipher', 'Great_St_Bernard_Pass', 'Ossipee_River', 'Great_Skua', 'Great_Tit', 'Great_Cormorant', 'Symphony_No._9_(Schubert)', 'Great_Pyrenees', 'Great_Duck_Island,_Maine', 'Great_Egret', 'Slayers_Great', 'Great_Frigatebird', 'Great_Bookham', 'Great_Chalfield_Manor', 'Great_Harrowden', 'St_Bartholomew-the-Great', 'Great_Synagogue_of_London'], ['Nirvana_(band)', 'Sacred_Love'], ['Music', '.music', 'Christmas_music', 'Film_score', 'Music_video', 'Record_producer', 'Music_video_game', 'Hip_hop_music', 'Music_festival', 'Popular_music', 'The_Mission_(soundtrack)', 'Music_of_Ireland', 'Music_education', 'West_Side_Story', 'Arabic_music', 'Music_of_Punjab', 'Music_therapy', 'Master_of_Music', 'Background_music', 'Music_venue', 'Music_College', 'French_classical_music', 'Le_tombeau_de_Couperin', 'Music_of_Spain', 'J-pop', 'Latin_American_music', 'Music_radio', 'Western_music_(North_America)', 'Bachelor_of_Fine_Arts_in_Music', 'Blue_(2009_film)'], ['Singing'], ['Singing', 'Singer-songwriter', 'Vocal_jazz', 'Human_voice', 'Mandopop', 'Elizabeth_Austin_(soprano)', 'Film_producer', 'Lead_vocalist', 'Playback_singer', \"Anita_O'Day\", 'Amaia_Montero', 'Belle_Perez', 'Girl_on_the_Run_(album)', 'Alizée', 'Cantopop', 'The_Futurist_(Robert_Downey_Jr._album)', 'Michael_Bolton', 'Pink_(singer)', 'Meat_Loaf', 'Lana_Lane', 'Wing_(singer)', 'Scott_Walker_(singer)', 'Dale_Watson_(singer)', 'Bilal_(American_singer)', 'Adele', 'Ksenija_Pajčin', 'Nick_Cave', 'Lisa_Mitchell', 'Gloria_Estefan', 'Child_singer']]\n",
      "(16,)\n",
      "(16,)\n",
      "(9, 30)\n",
      "(9, 2)\n",
      "(9,)\n",
      "(9,)\n",
      "dict_keys(['wiki'])\n",
      "[['Sanford_and_Son_Theme_(The_Streetbeater)', 'The_Times', 'The_Game_(mind_game)', 'British_humour', 'The_Twelve_Chairs', 'The_Sugarcubes', 'The_eXile', 'Zune', 'The_Wire', 'The_Beatles', 'MP3', 'The_Dana_Girls', 'Stadion_Lugovi', 'The_Secret_of_Monkey_Island', 'The_Banksia_Atlas', 'Laws_of_thermodynamics', 'Rocketeer', 'The_Satanic_Verses', 'Andromeda–Milky_Way_collision', 'The_Prisoner', 'Porsche_911', 'The_Catcher_in_the_Rye', 'The_Wall', 'Cassius_Marcellus_Clay_(politician)', 'The_KLF', 'The_Guardian', 'The_End_of_Time', 'Aral_Sea', 'The_Elder_Scrolls', 'Mary_Blair'], ['The_Louvre'], ['The_Louvre', 'Louvre_Pyramid', 'Venus_de_Milo'], ['Mountain_Dew', 'Virgin_Killer', 'Casomorphin', 'Rumic_World', 'Guarana', \"I_Can't_Believe_It's_Not_Butter!\", 'Welsh_phonology'], ['Isla_de_Mona', 'Mona,_Jamaica', 'Mona,_Utah', 'Mona_Township,_Ford_County,_Illinois', 'Mona_(band)', 'ASCII_art', 'Mona_Simpson_(The_Simpsons)', 'Hey!_Bo_Diddley', 'Mona_(opera)', 'Mona_Lisa', 'Mona_Best', 'Anglesey', 'Wario_(franchise)', 'Molly_Holly', 'Mona_(elephant)', 'Love_You_(album)', 'Mona_Mayfair', 'Hearts', 'Shift_JIS_art', 'Mona_Fitzalan-Howard,_11th_Baroness_Beaumont', 'Mona_Løseth', 'Mona_the_Virgin_Nymph', 'Mona_Mahmudnizhad', 'Mona_Tyndall', 'Mona_Islands', 'Mona_Thiba', 'Mona_Nemer', 'Mona_Polacca', 'Monas_(bishop_of_Milan)', 'Mona_Leydon'], ['Mona_Lisa', 'MS_Kungsholm_(1966)', 'When_the_World_Comes_Down', 'Antara_Biswas', 'Nighthawks', 'Union_Jack', 'Banksy', 'L.H.O.O.Q.', 'Queen_Victoria', 'Lisa_del_Giocondo', 'Marilyn_Monroe', 'Andy_Warhol', 'Dimension_X_(Teenage_Mutant_Ninja_Turtles)', 'La_Gioconda_(opera)', 'Leonardo_da_Vinci', 'Christina_Aguilera'], ['Apple_Lisa', 'Lisa_Simpson', 'Hurricane_Danielle_(2010)', 'Lisa,_Teleorman', 'Lisa_(musician)', 'Lisa_Kelly', 'Lisa_(singer)', 'Lisa,_Brașov', 'Lisa_Leslie', 'Weird_Science_(film)', 'Lisa_Faulkner', 'Lisa_Coleman_(musician)', 'Lisa_Marie_Presley', 'Lisa_Dingle', 'Lisa_Ling', 'Lisa_Kudrow', 'Lisa_(film)', 'The_Inspector_(1962_film)', 'Lisa_Hunter', 'Liza_Minnelli', 'Lisa_Fonssagrives', \"Lisa_D'Amato\", 'Lisa_Grimaldi', 'Lisa_Douglas', 'Lisa_Wilkinson', 'Lisa_Whelchel', 'Lisa_River', 'Lisa_Hannigan', 'Lisa_Murkowski', 'Lisa_Scott-Lee'], ['Sanford_and_Son_Theme_(The_Streetbeater)', 'The_Times', 'The_Game_(mind_game)', 'British_humour', 'The_Twelve_Chairs', 'The_Sugarcubes', 'The_eXile', 'Zune', 'The_Wire', 'The_Beatles', 'MP3', 'The_Dana_Girls', 'Stadion_Lugovi', 'The_Secret_of_Monkey_Island', 'The_Banksia_Atlas', 'Laws_of_thermodynamics', 'Rocketeer', 'The_Satanic_Verses', 'Andromeda–Milky_Way_collision', 'The_Prisoner', 'Porsche_911', 'The_Catcher_in_the_Rye', 'The_Wall', 'Cassius_Marcellus_Clay_(politician)', 'The_KLF', 'The_Guardian', 'The_End_of_Time', 'Aral_Sea', 'The_Elder_Scrolls', 'Mary_Blair'], ['Amazon_rainforest', 'Amazon_River', 'Pro_Wrestling_(Nintendo_Entertainment_System)', 'Amazon_Basin'], ['Amazon_River'], ['Amazon.com', 'Amazon_River', 'Amazon_Basin', 'Amazon_rainforest', 'Amazons', 'Amazon_Web_Services', 'Amazon_feminism', 'Amazons_(DC_Comics)', 'Volvo_Amazon', 'Kamen_Rider_Amazon', 'Amazon_(automobile)', 'Amazon_(1999_TV_series)', 'Amazon_MP3', 'Kamen_Rider_Amazon_(character)', 'Amazon_(2008_TV_series)', 'Amazon_parrot', 'Amazon_(1997_film)', 'Amazonas_(Brazilian_state)', 'Peruvian_Amazon', 'HMS_Amazon_(D39)', 'Amazônia_Legal', 'HMS_Amazon_(1795)', 'Amazon_Eve', 'HMS_Amazon_(1799)', 'HMS_Amazon_(1908)', 'HMS_Amazon_(F169)', 'Type_21_frigate', 'Amazon_bamboo_rat', 'Amazon_statue_types', 'Amazons_(novel)'], ['Amazon_River', 'Amazon_Basin'], ['River', 'Glossary_of_poker_terms', 'Hudson_River', 'Pampanga_River', 'Mississippi_River', 'River_Thames', 'Hackensack_River', 'Los_Angeles_River', 'San_Gabriel_River_(California)', 'Saint_Lawrence_River', 'River_Cam', 'Columbia_River', 'Chattahoochee_River', 'Rio_Grande', 'Charles_River', 'Brisbane_River', 'Delaware_River', 'Acushnet_River', 'Shenandoah_River', 'River_Shannon', 'Rocky_River_(Ohio)', 'Mad_River_(Ohio)', 'Ottawa_River', 'Danube', 'Chicopee_River', 'Río_de_la_Plata', 'Volkhov_River', 'South_Saskatchewan_River', 'Rivière_du_Loup', 'Saginaw_River'], ['Brazil', 'Brazil_national_football_team', 'Brazil,_Indiana', 'Brazil_national_basketball_team', \"Brazil_women's_national_football_team\", 'Empire_of_Brazil', 'Brazilian_Football_Confederation', 'Brazil_at_the_2008_Summer_Olympics', 'Brazilian_rock', \"Brazil_men's_national_volleyball_team\", 'Same-sex_marriage_in_Brazil', 'Brazil_national_rugby_union_team', 'Brazil_national_cricket_team', 'Brazilian_Carnival', 'Brazilian_Armed_Forces', 'Brazilian_Portuguese', 'Brazil_Davis_Cup_team', 'Brazil_at_the_1996_Summer_Olympics', 'A1_Team_Brazil', 'Brazil_national_baseball_team', 'Colonial_Brazil', 'Brazil_at_the_2008_Summer_Paralympics', \"Brazil_women's_national_volleyball_team\", \"Brazil_women's_national_basketball_team\", 'Brazil_national_futsal_team', 'Brazil_national_beach_soccer_team', 'Punk_in_Brazil', 'Brazil_at_the_2004_Summer_Olympics', 'Brazil_national_under-20_football_team', 'Aquarela_do_Brasil']]\n",
      "(15,)\n",
      "(15,)\n",
      "(14, 30)\n",
      "(14, 2)\n",
      "(14,)\n",
      "(14,)\n",
      "dict_keys(['wiki'])\n",
      "[['Donald_Duck', 'Donald', 'Donald,_Victoria', 'Shire_of_Donald', 'Donald,_Oregon', 'Stephen_Donald', 'Donald_Trump', 'Donald_Nixon', 'Donald,_British_Columbia', 'John_Donald_(Wisconsin_politician)', 'Allan_Donald', 'Donald_Rumsfeld', 'Donald_Knuth', 'Donald_Campbell', 'Luke_Donald', 'Donald,_Georgia', 'Donald_Newhouse', 'Mitchell_Donald', 'Donald_Braswell_II', 'Donald_Nally', 'Wally_Donald', 'Donald_Tsang', 'Fat_Albert_and_the_Cosby_Kids', 'Clan_Donald', 'David_Donald_(footballer)', 'Donald_Sinclair_(veterinary_surgeon)', 'There_are_known_knowns', 'Donald_Trump,_Jr.', 'Donald_\"Duck\"_Dunn', 'Donald_Sutherland'], ['Donald_Duck', 'Donald', 'Sony_Pictures_Television', 'Characters_of_Kingdom_Hearts', 'Ariel_Dorfman'], ['Donald_Duck'], ['Duck', 'Duck,_North_Carolina', 'Cotton_duck', 'Duck_the_Great_Western_Engine', 'Duck_(food)', 'Duck,_West_Virginia', 'Duck_(film)', 'Domestic_duck', 'DUKW', 'Oregon_Ducks', 'Jenny_Duck', 'Donald_Duck', 'Disco_Duck', \"Duck_(Alice's_Adventures_in_Wonderland)\", 'Duck_River_(Tennessee)', 'Duck_test', 'Tucker_Clellan', 'Duck_Records', 'Duck_(bridge)', 'Don_Manley', 'Duckie_(group)', 'Destroyer_Duck', 'Arthur_Duck', 'Cyberduck', 'Duck_Baker', 'Duck_Dodgers', 'Duck_(guitar)', 'Duck_MacDonald', 'Blue_Duck_(outlaw)', 'The_White_Duck'], ['Cartoon', 'Animated_cartoon', 'The_Transformers_(TV_series)', 'Beetlejuice_(TV_series)', 'Sonic_the_Hedgehog_(TV_series)', 'Battle_of_Cascina_(Michelangelo)', 'Raphael_Cartoons', 'Barnacle_Bill_(1930_film)', 'Teenage_Mutant_Ninja_Turtles_(1987_TV_series)', 'Robin_Hood_(1973_film)', 'Traditional_animation', 'The_Legend_of_Zelda_(TV_series)', 'Iran_newspaper_cockroach_cartoon_controversy', \"On_the_Internet,_nobody_knows_you're_a_dog\", 'Mr._Bean_(animated_TV_series)', 'Mr._Magoo', 'Animation', 'ProStars', \"Hulk_Hogan's_Rock_'n'_Wrestling\", 'Ulysses_31', 'Ed,_Edd_n_Eddy', 'ReBoot', 'The_Simpsons_Movie', 'Underdog_(TV_series)', 'Conan_the_Adventurer_(animated_series)', 'Mighty_Mouse', 'Animated_series', 'Pac-Man_(TV_series)', 'Yu-Gi-Oh!', 'Dungeons_&_Dragons_(TV_series)'], ['Character_(arts)', 'Pink_Panther_(character)', 'Casper_the_Friendly_Ghost', 'Jackie_Chan_Adventures', 'Strawberry_Shortcake', 'Olive_Oyl', 'Yosemite_Sam', 'Pluto_(Disney)', 'Howard_the_Duck', 'Wile_E._Coyote_and_The_Road_Runner', 'Underdog_(TV_series)', 'Homer_Simpson', 'Goofy', 'Mickey_Mouse', 'Tasmanian_Devil_(Looney_Tunes)', 'Cleveland_Brown', 'Cheburashka', 'The_Fabulous_Furry_Freak_Brothers', 'Donald_Duck', 'Cartoon', 'Felix_the_Cat', 'Mighty_Mouse'], ['Character_(arts)', 'Moral_character', 'Character_(computing)', 'Chinese_characters', 'Character_structure', 'Character_actor', 'Player_character', 'Kanji', 'Character_(symbol)', 'Stephen_Colbert_(character)', 'Spider-Man', 'Hulk_(comics)', 'Character_dance', 'Ellen_Ripley', 'Ghost_Rider_(comics)', 'Thulsa_Doom', 'Peter_Pan', 'Horatio_(Hamlet)', 'Pink_Panther_(character)', 'Count_Dracula', 'Count_von_Count', 'Decimal_mark', 'Characters_of_Final_Fantasy_VIII', 'Cheeta', 'Terminator_(character)', 'Horatio_Hornblower', 'Count_Saint-Germain_(vampire)', 'Murmur_(Marvel_Comics)', 'Utopia_(Doctor_Who)', 'Kayfabe']]\n",
      "(8,)\n",
      "(8,)\n",
      "(7, 30)\n",
      "(7, 2)\n",
      "(7,)\n",
      "(7,)\n",
      "dict_keys(['wiki'])\n",
      "[['Hayao_Miyazaki'], ['Hayao_Miyazaki'], ['Hayao_Miyazaki', 'Tsutomu_Miyazaki', 'Manabu_Miyazaki', 'Ui_Miyazaki', 'Manabu_Miyazaki_(photographer)', 'Princess_Mononoke', 'Spirited_Away'], ['Cobalt', 'Atoy_Co', 'Co_Hoedeman', 'Co_Verkade', 'E._F._Hodgson_Company', 'Cω', 'West_End_Watch_Co.', 'Nguyen_Huu_Co', 'Co_Adriaanse', 'Co_Prins', 'Cor_people', 'Co-operative_Commonwealth_Federation', 'Corvette', 'Typhoon_Conson_(2004)', 'Co-operative_federalism', 'Typhoon_Conson_(2010)', 'Co-operatives_UK', 'Co-op_City,_Bronx', 'Co-operative_economics', 'Co-sleeping', 'Trimethoprim/sulfamethoxazole', 'Co-codamol', 'Co-counselling', 'Co-operative_Party', 'Cthulhu_Mythos_deities', 'National_League_1', 'Co-Counselling_International', 'Company_(military_unit)', 'Cobalt-60', 'Xu_(surname)'], ['AAR_wheel_arrangement', 'Hyphen-minus', 'Plus_and_minus_signs', 'Hyphen', 'Syntonic_comma', 'Sarasvati_River', 'Victorian_era'], ['Entrepreneurship', 'Founder_Group', 'Founder_Technology', 'William_Booth', 'Founding_Fathers_of_the_United_States', 'Muhammad_Ali_Jinnah', 'William_Berczy', 'Flatfish', 'Walter_Landor', 'Anthony_Joseph_Drexel', \"Founder's_Building\", 'Thomas_Paine', 'Joseph_Smith', 'Laminitis', 'Claude_Vorilhon', 'Chief_executive_officer', 'Scottish_Church_College', 'Bridges_TV', 'Green_Shield_Stamps', 'Robert_Parris_Moses', 'Korea_Hapkido_Federation', 'Ravidassia_religion', 'Tang_Soo_Do', 'Dominion_(Star_Trek)'], ['Studio', 'Studio_art', 'Studio_(band)', 'Dell_Studio', 'Album', 'University_of_Bologna', 'Record_label', 'Apple_Studio_Display', 'Recording_studio', 'Studio_apartment', 'Actors_Studio', 'Fleischer_Studios', 'Television_studio', 'Oracle_Solaris_Studio', 'Black_Rock_Studio', 'Namco_Tales_Studio', 'Shaw_Brothers_Studio', 'Asobo_Studio', 'Site_Studio', 'First_Act_Guitar_Studio', 'Sega_Racing_Studio', 'Circle_Studio', 'European_Integration_Studio', 'Studio_Gigante', 'Tile_Studio', 'Scottish_Cartoon_Art_Studio', 'Tokkun_Studio', 'Cinimod_Studio', 'Clover_Studio', 'Ravn_Studio'], ['Arrietty', 'Ghibli_Museum'], ['Ghibli_Museum'], ['Michael_Van_Valkenburgh', 'Louis_Philippe_(musician)', 'Fatemeh_Motamed-Arya', 'Paulo_Coelho', 'Daniel_Lanois', 'Ken_Hutcherson', 'London_School_of_Economics', 'Carlos_Monsiváis', 'Peter_Drucker'], ['Anime', 'Pokémon_(anime)', 'Witchblade_(anime)', 'Speed_Racer', 'Metropolis_(2001_film)', 'Bleach_(manga)', 'Yu-Gi-Oh!_Duel_Monsters_GX', 'Mega_Man_Star_Force_(anime)', 'Mobile_Suit_Gundam', 'Toei_Animation', 'Neon_Genesis_Evangelion', 'Hellsing', 'Anime_convention', 'Dragon_Ball', 'Monogatari_(series)', 'Castle_in_the_Sky', 'Blue_rose', 'Emma_(manga)', 'Stellvia', 'Neon_Genesis_Evangelion_(franchise)', 'Digimon', 'Shizuku-chan', 'Haruhi_Suzumiya', 'Battle_Angel_Alita', 'Yatterman', 'Akira_(film)', 'Bio_Booster_Armor_Guyver', 'Robot_Carnival', 'Steamboy', 'Geofront'], ['Film_director', 'FileMaker', 'Filmmaker_(film)'], ['Sanford_and_Son_Theme_(The_Streetbeater)', 'The_Times', 'The_Game_(mind_game)', 'British_humour', 'The_Twelve_Chairs', 'The_Sugarcubes', 'The_eXile', 'Zune', 'The_Wire', 'The_Beatles', 'MP3', 'The_Dana_Girls', 'Stadion_Lugovi', 'The_Secret_of_Monkey_Island', 'The_Banksia_Atlas', 'Laws_of_thermodynamics', 'Rocketeer', 'The_Satanic_Verses', 'Andromeda–Milky_Way_collision', 'The_Prisoner', 'Porsche_911', 'The_Catcher_in_the_Rye', 'The_Wall', 'Cassius_Marcellus_Clay_(politician)', 'The_KLF', 'The_Guardian', 'The_End_of_Time', 'Aral_Sea', 'The_Elder_Scrolls', 'Mary_Blair'], ['Alps', 'Alpine,_Texas', 'Alpine,_California', 'Alpine_(automobile)', 'Alpine_(email_client)', 'Alpine,_New_Jersey', 'Alpine_County,_California', 'Alpine,_Arizona', 'Alpine_Electronics', 'Alpine,_Mendocino_County,_California', 'Alpine,_Utah', 'Alpine,_Los_Angeles_County,_California', 'Alpine_race', 'Alpine_skiing', 'Alpine,_Virginia', 'Simca_1307', 'Alpine,_Wyoming', 'Alpine,_New_South_Wales', 'Alpine,_Arkansas', 'Alpine,_Alaska', 'Alpine,_Talladega_County,_Alabama', 'Alpine,_Oregon', 'Alpine,_DeKalb_County,_Alabama', 'Alpine,_Indiana', 'Alpine_National_Park', 'Alpine_(G.I._Joe)', 'Alpine_(former_settlement),_California', 'Alpine,_Colorado_(ghost_town)', 'Alpine_(goat)', 'Alpine,_Tennessee'], ['Alpine_ibex'], ['Capra_(genus)', 'Ibex_(band)', 'IBEX_35', 'Siberian_ibex', 'Ibex_(vehicle)', 'Nilgiri_tahr', 'Ibex_Outdoor_Clothing', 'Ibex_Airlines', 'Ibex_Valley', 'Alpine_ibex', 'Ibex_Mountain', 'USS_Ibex_(IX-119)', 'Nubian_ibex', 'Spanish_ibex', 'Ibex_Cave'], ['Switzerland', 'Switzerland_national_football_team', 'Swiss_Football_Association', \"Switzerland_men's_national_ice_hockey_team\", 'Switzerland_in_the_Eurovision_Song_Contest', 'Registered_partnership_in_Switzerland', 'A1_Team_Switzerland', 'Switzerland_Davis_Cup_team', 'Switzerland_national_baseball_team', 'Switzerland_at_the_2006_Winter_Olympics', 'Military_of_Switzerland', 'Switzerland_at_the_2010_Winter_Olympics', 'Switzerland_national_under-21_football_team', 'Swiss_Hitparade', \"Switzerland_women's_national_football_team\", 'Switzerland_national_rugby_union_team', 'Ligue_Nationale_de_Basketball_(Switzerland)', 'Switzerland_national_cricket_team', 'Swiss_Super_League', 'Switzerland_at_the_2008_Summer_Olympics', 'Switzerland_County,_Indiana', 'Switzerland_national_beach_soccer_team', 'Immigration_from_the_former_Yugoslavia_to_Switzerland', 'Switzerland_at_the_1992_Summer_Olympics', 'Flag_of_Switzerland', 'Switzerland_national_basketball_team', 'Switzerland_Fed_Cup_team', \"Switzerland_women's_national_ice_hockey_team\", 'Switzerland_national_under-17_football_team', 'Switzerland_(album)'], ['Apostrophe', 'Foot_(unit)', 'Minute', 'Shadda', 'Vyākaraṇa', 'Minute_of_arc', 'Āstika_and_nāstika', \"Can't_Hardly_Wait\", 'Darśana', 'Hindu_astrology', 'Shin_(letter)', 'Waw_(letter)', 'Big_Brother_(U.S.)', 'Shiksha', 'Kammaṭṭhāna', 'Penal_labour', 'Vedanga', 'Rigveda', 'Mahātmā', 'Yantra', 'Kasina', \"Apostrophe_(')\", 'Pariśiṣṭa', 'Svādhyāya'], ['Sulfur', 'S', 'Silurian', 'Ś', 'Sat_Parashar', 'S_V_S_Rama_Rao', 'S_E_A_Holdings', 'S_G_Thakur_Singh', 'Š', 'S_John_Massoud', 'Ŝ', 'S._U._Hastings', 'S_Club', 'S._K._Venkataranga', 'S._B._Tambe', 'Sivaramakrishnan_Murali', 'S_(programming_language)', 'UK_railway_stations_–_S', 'Southern_Hemisphere', 'ATC_code_S', 'Simplified_Chinese_characters', 'Shot_(ice_hockey)', 'United_States_District_Court_for_the_Southern_District_of_Iowa', 'S-type_star', 'Sense_(molecular_biology)', 'S-type_asteroid', '1996_Italian_Indoor_–_Singles', '1996_Trofeo_Conde_de_Godó_–_Singles', \"1996_French_Open_–_Men's_Singles\", 'Haplogroup_S-M230'], ['Capture_of_HMS_Frolic', 'Carlo_Rubbia', 'Albert_Einstein', 'Bolton', 'Celebrity', 'Patty_Murray', 'Fruits_Basket', 'Voodoo_Macbeth', 'John_Mauchly', 'John_Berger', 'Pirates_versus_Ninjas', 'Jon_Burge', 'Lenny_Waronker', 'Big_Mac', 'Lists_of_named_passenger_trains', 'Time–space_compression', 'Pale_Male', \"The_Sorcerer's_Apprentice\", 'The_3AM_Girls', 'Cornel_West', 'Epimetheus_(moon)', 'Danny_Simpson', 'Serial_comma', 'Lee_Baca', 'Ellen_von_Unwerth', 'Kevin_B._MacDonald', 'Osama_bin_Laden', 'Canopic_jar', 'Sydney_to_Hobart_Yacht_Race', 'Brian_Bonsall'], ['Animal', 'Animal_(Muppet)', 'Road_Warrior_Animal', 'Animal_(Pearl_Jam_song)', 'Animal_(Def_Leppard_song)', 'Animal_(R.E.M._song)', 'Animal_Collective', 'Animal_(video_game)', 'Animal_(2005_film)', 'Animal_(Jebediah_song)', 'Animals_in_Buddhism', 'Animal_(Kesha_album)', 'Miike_Snow_(album)', 'Animal_(Animosity_album)', 'Animal_(clothing)', 'Animal_(Motor_Ace_album)', 'Animal!', 'The_Animals', 'Animal_(book)', 'W.A.S.P.', 'Animal_(restaurant)', 'Kay_Adshead', 'Human_Animal', 'A.N.I.M.A.L.', 'Animal_Kingdom_(band)', 'Animal_Prufrock', 'Animals_(Kevin_Ayers_song)', 'Animal_(2001_film)', 'Frank_Bialowas', 'Animals_(Nickelback_song)'], ['Grazing', 'Pastoralism', 'Pasture', 'Animal_husbandry'], ['Cattle', 'Cow_tipping', 'Spherical_cow', 'CowParade', 'Beef', 'Angus_cattle', 'Cows_(band)', 'Zebu', 'Cattle_in_religion', 'Limousin_(cattle)', 'Aurochs', 'Laurasiatheria', 'Dairy_cattle', 'Livestock', 'Highland_cattle']]\n",
      "(42,)\n",
      "(42,)\n",
      "(23, 30)\n",
      "(23, 2)\n",
      "(23,)\n",
      "(23,)\n",
      "{'tokens': {'tokens': tensor([[  101,  3000,  2003,  2284,  1999,   103,  1012,   102,  2745,   103,\n",
      "          2003,  1037,  2307,  2189,  3220,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996, 25110,  3397,  1996, 13813,  7059,   102,  1996,  9733,\n",
      "          2314,  2003,  1999,  4380,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6221,  9457,  2003,  1037,  9476,  2839,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 10974,  7113,  2771,  3148, 18637,  2003,  1996,  2522,  1011,\n",
      "          3910,  1997,  2996,  1043,  4048, 16558,  2072,  1998,  1037,  8228,\n",
      "          8750,  2143, 22626,   102,  1996, 10348, 21307, 10288,  2003,  2028,\n",
      "          1997,  5288,  1005,  1055,  2087,  3297,  4111,  2247,  2049, 15400,\n",
      "         17188,   102]])}, 'segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'candidates': {'wiki': {'candidate_entity_priors': tensor([[[8.1099e-01, 1.5043e-02, 8.7729e-03,  ..., 5.0884e-03,\n",
      "          5.0282e-03, 4.9880e-03],\n",
      "         [5.0453e-01, 1.3937e-01, 1.3937e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [4.7059e-01, 1.1765e-01, 8.4034e-02,  ..., 8.4034e-03,\n",
      "          8.4034e-03, 8.4034e-03],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.2065e-01, 7.6787e-02, 7.1482e-02,  ..., 1.4994e-02,\n",
      "          1.4645e-02, 1.4296e-02],\n",
      "         [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [9.8974e-01, 9.5206e-03, 7.3751e-04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[3.1306e-01, 1.9588e-01, 6.3618e-02,  ..., 5.0287e-03,\n",
      "          4.6779e-03, 4.3271e-03],\n",
      "         [7.9087e-01, 1.9772e-01, 6.7049e-03,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [7.9515e-01, 5.4376e-02, 4.8576e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [4.8283e-01, 3.3217e-02, 2.5683e-02,  ..., 1.6184e-02,\n",
      "          1.6184e-02, 1.6184e-02],\n",
      "         [6.6404e-01, 3.3552e-01, 2.1894e-04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [9.6251e-01, 6.9675e-03, 6.3039e-03,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]), 'candidate_entities': {'ids': tensor([[[ 61415, 186436, 156993,  ..., 348909, 228856, 408537],\n",
      "         [217306, 208363, 195501,  ...,      0,      0,      0],\n",
      "         [155492, 442608, 446511,  ..., 455031,  52372, 201398],\n",
      "         ...,\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0]],\n",
      "\n",
      "        [[420164, 119824, 349042,  ..., 268223,  21878, 276027],\n",
      "         [302306,      0,      0,  ...,      0,      0,      0],\n",
      "         [302306, 434946, 236959,  ...,      0,      0,      0],\n",
      "         ...,\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0]],\n",
      "\n",
      "        [[200988, 217004, 167163,  ..., 244593, 293321, 169507],\n",
      "         [200988, 217004,  13208,  ...,      0,      0,      0],\n",
      "         [200988,      0,      0,  ...,      0,      0,      0],\n",
      "         ...,\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0],\n",
      "         [     0,      0,      0,  ...,      0,      0,      0]],\n",
      "\n",
      "        [[115868,      0,      0,  ...,      0,      0,      0],\n",
      "         [115868,      0,      0,  ...,      0,      0,      0],\n",
      "         [115868, 244835, 417680,  ...,      0,      0,      0],\n",
      "         ...,\n",
      "         [125905, 378976,  83899,  ..., 468084, 435596, 129585],\n",
      "         [312981, 341166, 258051,  ...,      0,      0,      0],\n",
      "         [448088,  34267,  21434,  ...,      0,      0,      0]]])}, 'candidate_spans': tensor([[[ 1,  1],\n",
      "         [ 3,  3],\n",
      "         [ 6,  6],\n",
      "         [ 8,  8],\n",
      "         [12, 12],\n",
      "         [12, 13],\n",
      "         [13, 13],\n",
      "         [13, 14],\n",
      "         [14, 14],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1]],\n",
      "\n",
      "        [[ 1,  1],\n",
      "         [ 1,  2],\n",
      "         [ 2,  2],\n",
      "         [ 3,  3],\n",
      "         [ 5,  5],\n",
      "         [ 5,  6],\n",
      "         [ 6,  6],\n",
      "         [ 8,  8],\n",
      "         [ 8,  9],\n",
      "         [ 8, 10],\n",
      "         [ 9,  9],\n",
      "         [ 9, 10],\n",
      "         [10, 10],\n",
      "         [13, 13],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1]],\n",
      "\n",
      "        [[ 1,  1],\n",
      "         [ 1,  2],\n",
      "         [ 1,  5],\n",
      "         [ 2,  2],\n",
      "         [ 5,  5],\n",
      "         [ 5,  6],\n",
      "         [ 6,  6],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1],\n",
      "         [-1, -1]],\n",
      "\n",
      "        [[ 1,  2],\n",
      "         [ 1,  5],\n",
      "         [ 3,  5],\n",
      "         [ 8,  8],\n",
      "         [ 9,  9],\n",
      "         [10, 10],\n",
      "         [12, 12],\n",
      "         [12, 16],\n",
      "         [13, 16],\n",
      "         [19, 19],\n",
      "         [20, 20],\n",
      "         [21, 22],\n",
      "         [24, 24],\n",
      "         [25, 25],\n",
      "         [25, 27],\n",
      "         [26, 27],\n",
      "         [31, 31],\n",
      "         [32, 32],\n",
      "         [33, 33],\n",
      "         [35, 35],\n",
      "         [36, 36],\n",
      "         [39, 39],\n",
      "         [40, 40]]]), 'candidate_segment_ids': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}}}\n",
      "wordnet entity_attention_probs are equal: True\n",
      "Segment ids are equal: True\n",
      "Candidate entity_priors are equal: True\n",
      "Candidate entities ids are equal: True\n",
      "Candidate span are equal: True\n",
      "Candidate segments_ids are equal: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kb/kb/custom_tokenizer/custom_tokenizer.py:125: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(dicts['candidates']['wiki']['candidate_entity_priors'].shape)\n"
     ]
    }
   ],
   "source": [
    "#Create test set\n",
    "sentences = [[\"Paris is located in [MASK].\", \"Michael [MASK] is a great music singer\"],\n",
    "            [\"The Louvre contains the Mona Lisa\", \"The Amazon river is in Brazil\"],\n",
    "            \"Donald Duck is a cartoon character\",\n",
    "            [\"Hayao Miyazaki is the co-founder of Studio Ghibli and a renowned anime filmaker\",\n",
    "            \"The Alpine ibex is one of Switzerland's most famous animal along its grazing cows\"]]\n",
    "            \n",
    "def batchifier_equal(original_batcher,custom_batcher,test_sentences):\n",
    "    for original_batch,custom_batch in zip(original_batcher.iter_batches(test_sentences,verbose=False),custom_batcher.iter_batches(test_sentences,verbose=False)):\n",
    "        \n",
    "        print(original_batch)\n",
    "        \n",
    "        print(f\"wordnet entity_attention_probs are equal: {torch.equal(original_batch['tokens']['tokens'], custom_batch['tokens']['tokens'])}\")\n",
    "        #Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "        #shape: (batch_size,max_seq_len)\n",
    "        print(f\"Segment ids are equal: {torch.equal(original_batch['segment_ids'],custom_batch['segment_ids'])}\")\n",
    "\n",
    "        original_wiki_kb = original_batch['candidates']['wiki']\n",
    "        custom_wiki_kb = custom_batch['candidates']['wiki']\n",
    "\n",
    "        print(f\"Candidate entity_priors are equal: {torch.equal(original_wiki_kb['candidate_entity_priors'],custom_wiki_kb['candidate_entity_priors'])}\")\n",
    "        print(f\"Candidate entities ids are equal: {torch.equal(original_wiki_kb['candidate_entities']['ids'],custom_wiki_kb['candidate_entities']['ids'])}\")\n",
    "        print(f\"Candidate span are equal: {torch.equal(original_wiki_kb['candidate_spans'],custom_wiki_kb['candidate_spans'])}\")\n",
    "\n",
    "        #For each sentence entity, indicate to which segment ids it corresponds to\n",
    "        print(f\"Candidate segments_ids are equal: {torch.equal(original_wiki_kb['candidate_segment_ids'],custom_wiki_kb['candidate_segment_ids'])}\")\n",
    "    \n",
    "\n",
    "batchifier_equal(original_batcher,custom_batcher,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: dict_keys(['tokens', 'segment_ids', 'candidates'])\n",
      "Tokens shape torch.Size([4, 42])\n",
      "Segment ids shape: torch.Size([4, 42])\n",
      "Candidate entity_priors shape: torch.Size([4, 23, 30])\n",
      "Candidate entities ids shape: torch.Size([4, 23, 30])\n",
      "Candidate span shape: torch.Size([4, 23, 2])\n",
      "Candidate segments_ids shape: torch.Size([4, 23])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_set = []\n",
    "# for sentence in sentences:\n",
    "for batch in original_batcher.iter_batches(sentences, verbose=False):\n",
    "    print(f\"Batch: {batch.keys()}\") #Batch contains {tokens,segment_ids,candidates}\n",
    "    #tokens: Tensor of tokens indices (used to idx an embedding) => because a batch contains multiple\n",
    "    #sentences with varying # of tokens, all tokens tensors are padded with zeros \n",
    "    #shape: (batch_size (#full_sentences(if two sentences => link them with NSP)), max_seq_len)\n",
    "    #print(batch['tokens'])#dict with only 'tokens'\n",
    "    print(f\"Tokens shape {batch['tokens']['tokens'].shape}\")\n",
    "    #Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "    #shape: (batch_size,max_seq_len)\n",
    "    print(f\"Segment ids shape: {batch['segment_ids'].shape}\")\n",
    "\n",
    "    #Dict with only wordnet\n",
    "    #Candidates: stores for multiple knowledge base, the entities detected using this knowledge base\n",
    "    wiki_kb = batch['candidates']['wiki']\n",
    "    # print(f\"Wordnet kb: {wordnet_kb.keys()}\")\n",
    "\n",
    "    #Stores for each detected entities, a list of candidate KB entities that correspond to it\n",
    "    #Priors: correctness probabilities estimated by the entity linker (sum to 1 (or 0 if padding) on axis 2)\n",
    "    #Adds 0 padding to axis 1 when there is less detected entities in the sentence than in the max sentence\n",
    "    #Adds 0 padding to axis 2 when there is less detected KB entities for an entity in the sentence than in the max candidate KB entities entity\n",
    "    #shape:(batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entity_priors shape: {wiki_kb['candidate_entity_priors'].shape}\")\n",
    "    #Ids of the KB candidate entities + 0 padding on axis 1 or 2 if necessary\n",
    "    #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "    print(f\"Candidate entities ids shape: {wiki_kb['candidate_entities']['ids'].shape}\")\n",
    "    #Spans of which sequence of tokens correspond to an entity in the sentence, eg: [1,2] for Michael Jackson (both bounds are included)\n",
    "    #Padding with [-1,-1] when no more detected entities\n",
    "    #shape: (batch_size, max # detected entities, 2)\n",
    "    print(f\"Candidate span shape: {wiki_kb['candidate_spans'].shape}\")\n",
    "\n",
    "    #For each sentence entity, indicate to which segment ids it corresponds to\n",
    "    print(f\"Candidate segments_ids shape: {wiki_kb['candidate_segment_ids'].shape}\")\n",
    "    #break\n",
    "\n",
    "\n",
    "\n",
    "    # \n",
    "    #shape: (batch_size, max # detected entities)\n",
    "    # model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nInput\\n\")\n",
    "print(f\"Batch: {batch.keys()}\") #Batch contains {tokens,segment_ids,candidates}\n",
    "#tokens: Tensor of tokens indices (used to idx an embedding) => because a batch contains multiple\n",
    "#sentences with varying # of tokens, all tokens tensors are padded with zeros \n",
    "#shape: (batch_size (#sentences), max_seq_len)\n",
    "#print(batch['tokens'])#dict with only 'tokens'\n",
    "print(f\"Tokens shape {batch['tokens']['tokens'].shape}\")\n",
    "#Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "#shape: (batch_size,max_seq_len)\n",
    "print(f\"Segment ids shape: {batch['segment_ids'].shape}\")\n",
    "\n",
    "#Dict with only wordnet\n",
    "#Candidates: stores for multiple knowledge base, the entities detected using this knowledge base\n",
    "wordnet_kb = batch['candidates']['wordnet']\n",
    "print(f\"Wordnet kb: {wordnet_kb.keys()}\")\n",
    "\n",
    "#Stores for each detected entities, a list of candidate KB entities that correspond to it\n",
    "#Priors: correctness probabilities estimated by the entity linker (sum to 1 (or 0 if padding) on axis 2)\n",
    "#Adds 0 padding to axis 1 when there is less detected entities in the sentence than in the max sentence\n",
    "#Adds 0 padding to axis 2 when there is less detected KB entities for an entity in the sentence than in the max candidate KB entities entity\n",
    "#shape:(batch_size, max # detected entities, max # KB candidate entities)\n",
    "print(f\"Candidate entity_priors shape: {wordnet_kb['candidate_entity_priors'].shape}\")\n",
    "#Ids of the KB candidate entities + 0 padding on axis 1 or 2 if necessary\n",
    "#shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "print(f\"Candidate entities ids shape: {wordnet_kb['candidate_entities']['ids'].shape}\")\n",
    "#Spans of which sequence of tokens correspond to an entity in the sentence, eg: [1,2] for Michael Jackson (both bounds are included)\n",
    "#Padding with [-1,-1] when no more detected entities\n",
    "#shape: (batch_size, max # detected entities, 2)\n",
    "print(f\"Candidate span shape: {wordnet_kb['candidate_spans'].shape}\")\n",
    "\n",
    "#For each sentence entity, indicate to which segment ids it corresponds to\n",
    "#shape: (batch_size, max # detected entities)\n",
    "print(f\"Candidate segments_ids shape: {wordnet_kb['candidate_segment_ids'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9797c430b9b8ca5d4cad34220fe2c597e42ba7691ef10261f4554305aef3ef0a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('knowbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
